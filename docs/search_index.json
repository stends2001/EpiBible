[["index.html", "Epidemiology in R: A Hands-on Approach Epidemiology in R: A Hands-on Approach", " Epidemiology in R: A Hands-on Approach Sten de Schrijver 2025-08-23 Epidemiology in R: A Hands-on Approach Please realize that this reader is work in progress. Some sections are not finished (nor started), work has not yet been cited properly, and I need to make the code a lot more comprehensive in some areas. I will also add many exercises, perhaps in a sort of supplementary reader, and I need to remove some chunks’ output (for example, sometimes when loading a package I have a big warning that some function names are being overwritten) that I need to get rid off still. "],["preface.html", "Preface", " Preface Hi, and welcome! I truly appreciate you taking the time to look over this work of mine. This digital book is designed to serve as a reference covering essential concepts that everyone working in epidemiology should know. We will explore some (minimal) theoretical epidemiology and examine various epidemiological concepts through real-world disease scenarios. After introducing how R works and how it can be applied across many areas of public health, we will discuss how to access and manage epidemiological data. Finally, we’ll bring everything together and study an extensive dataset that relates to the dengue fever in Brazil, so that you anyone, regardless of background, becomes confident enough working with R to learn more at their own pace about this fascinating field of science. Public health is a prime example of a multidisciplinary area in which diverse scientific fields converge. The One Health approach in particular, not only connects different disciplines but also encourages multiple perspectives on the same issue. For example, a medical doctor may view a mosquito as a vector transmitting disease, while an ecologist might see it as an invasive species indicating changing habitat suitability, perhaps due to global warming. Although One Health experts recognize the importance of interdisciplinary collaboration and education, this integration often falls short when it comes to computers and data. This is precisely the gap this material aims to bridge. As this is a first draft intended as background material for a future course, I warmly welcome your feedback! Please feel free to share your thoughts by emailing me. "],["about-the-author.html", "About the Author", " About the Author As a graduate from a bachelor’s degree in Nanobiology (joint degree of the Techinical University in Delft and ERasmus Medical School, the Netherlands) and the MSc. Erasmus Mundus Joint Degree in Infectious Diseases and One Health, I have a solid background in molecular biology, infection biology, public health, epidemiology and data analytics. The fields specifically, I am now integrating on a daily basis during my PhD (Robert Koch Institut, Berlin) in artificial intelligence in public health. On a daily basis I look at the prevalence of vaccine-preventable diseases, and try to model dynamics and occurances of these diseases. During my studies, I have seen that people understand more and more the necessity of learning different skills from different fields, to become a more “well - rounded” scientist. While advocating for this, I have also experienced the hesitancy of both students and specialists to take up new skills of the digital and the mathematical area. Especially in the information age that we live in, I find it inexcplicable that students graduating from degrees in public health have practically no experience with working with data. "],["introduction-to-epidemiology.html", "Chapter 1 Introduction to Epidemiology", " Chapter 1 Introduction to Epidemiology Coming soon … "],["working-with-this-book.html", "Chapter 2 Working with this book 2.1 What is bookdown? 2.2 Code snippets 2.3 A Hand’s-on Approach", " Chapter 2 Working with this book Before diving into the coding content, this chapter explains how this document is organized and how you can make the most of the examples and code snippets provided throughout this reader. 2.1 What is bookdown? This book is written using bookdown, a package in R that helps create digital books and long-form documents using R Markdown. Bookdowns allow us to do multiple things, in one clean and organized document: combine text and executable R code in one place automatically generates formatted output like HTML, PDF or e-books Cross-reference chapters, figures tables and equations provide code snippets that any reader can copy paste into executable code chunks for easy practice. 2.2 Code snippets Throughout this book you will find code chunks like the one on the next few lines. In grey and italic you will see comments, denoted by starting with a hashtag (#). Any normal looking text is executable code. Upon generation of this document, all code chunks that are written are executed. The code found in these chunks can be copied, and pasted into your own R. To do so, hover over the top right corner of the chunk, and the option to copy should pop up. # This is a comment print(&quot;this is executable code!&quot;) ## [1] &quot;this is executable code!&quot; Below each chunk, you find another grey background box, which holds the output of that chunk. These boxes contain text that start with ## [1] and so on. This is the way of R to show the index of the first element in that row of printed output. It helps you orient yourself in long outputs. If we print 100 numbers for example, you’ll see a number of these indices showing up, by which R tells us that the 73rd output was the number 73, etc. print(1:100) ## [1] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 ## [19] 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 ## [37] 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 ## [55] 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 ## [73] 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 ## [91] 91 92 93 94 95 96 97 98 99 100 2.3 A Hand’s-on Approach You can’t learn a programming language from reading, similarly to how you can’t learn a language by only reading. You need to speak, and to write as well, to make sure you find our own way of speaking the language, and so that you can practice what you learned in theory! We will be following the same philosophy. For now, this document is mostly meant as a reader, but soon enough there will be exercises in here as well, at the end of each chapter, so that you get to practice what you learned. Text formats in this Reader There’s a few common text-formats. I’ll be using these: bold: terminology italic: variables in functions inline code: functions "],["getting-started-with-r.html", "Chapter 3 Getting Started with R 3.1 Installing R and Rstudio 3.2 Starting to code 3.3 R Projects and Organization", " Chapter 3 Getting Started with R 3.1 Installing R and Rstudio R is not a computer program like Microsoft Word. Instead, it is a computer language, just like Python, Matlab, Java and many others. You can tell your computer to execute commands, by writing the commands in such a computer language. RStudio is the interface, an application, that functions as a tool to help you write, understand and organize your R code. In order to get R up and running on your system we therefore need to install both R and Rstudio. Follow the steps on the Rstudio-website specifically for your system (Linux, Mac, Windows). After installation, you can open the application RStudio and run R code in it. You’ll notice your screen in Rstudio to be divided into 4 sections: script editor (top left); this is where we will do most work. We will write and edit code in this place console (bottom left); this is where we execute code. while you can run R commands here directly, we will execute code here written in the script editor. environment pane (top right); shows all the current variables, datasets and anything else in the working memory file pane (bottom right); allows you to browse over files The R language is very large and diverse. Any user of R can extend the language with new functions and new tools, which come together in the shape of a package. There are about 20 thousand (!) R packages available, each of which has their own specialty. When some clever person creates a new epidemiological model and wants it to become available for the community, they can upload it on R and make it available for us. While the volume of packages make it sound like you can easily lose track of everything that’s out there, there’s only a handful of packages that really everyone tends to use. For example, the Tidyverse is a collection of packages that many people working with R have extensive experience with. We will also be using it a lot in this course, specifically the packages ggplot2 (for visualization) dplyr (for data manipulation), readr (for reading datafiles) and tidyr (for reshaping dataframes). We will go through installing and loading packages, and writing functions later in this course. For now we can get started on some very simple things in R, to get you familiar with doing some coding! 3.2 Starting to code The print() function is a very basic and useful one, which allows you to often check some things quickly. On the first line in the script editor (left top pane), go and write the following line: print(&#39;hello world!&#39;) ## [1] &quot;hello world!&quot; with your cursor on the right side of that line, press the “run”-button. You will see the output in the console where it now shows in yellow what has been executed, and in white the output is shown. R is great for doing some easy mathematics. Let’s do some calculations and print the output. Here we have multiple lines of code, that we need to execute in that order for it to work. We can’t first run the print, and then do the calculations. We could do that line by line, so press the run button each time. Alternatively, you can select all these lines, and then press the run button. Another option is to use the shortcut, which for Windows users is: CTRL + Enter. This is the equivalent of the Run button. For the execution of an entire code chunk, you can also run CTRL + SHIFT + ENTER x = 280 y = 10 z = 0.4 product = x/y*z print(product) ## [1] 11.2 We will discuss more R syntax, and more things to code, at a later point. For now let’s leave it at this and talk a bit about how we organize our code. 3.3 R Projects and Organization We can save our code in an R script, which is the most basic R file. To do that, we go CTRL + S and we can navigate to the location to save our file. Besides this R file (extension: .R) we have a couple of other options, including an R Markdown file. I am a huge fan of these. If you’re familiar with Python; R Markdown - files are the R equivalent of a Jupyter Notebook file. They allow you to write code in the same file as nicely organized text. You can put in graphs that look really nice, add citations and a bibliography, add chapters, and so much more. This whole textbook is actually running in a handful of R Markdown files. Eventually, I want to write a small project with you in an R Markdown as well, though for now we can get started with R scripts. To organize our R code properly, we shouldn’t work with individual R scripts. It is much better to have a specific environment for your R code, in which you store your analysis (scripts), your data and your results and figures. Something I struggle a lot with myself is with directories. A directory is a certain folder’s location, a way of pinpointing where R is working (the working directory). This is important to know, because when we tell R where to find a certain dataset on our computer, the computer needs to know where to look, and it starts looking relative to the working directory. To have one nicely organized environment, and have set working directories, we have something called an *R project**. This is exactly an environment to bundle different R files in a separate folder, where the working directory is automatically set to the root of the project; the location of the project file. We create an R project through the following steps: File &gt; New Project A window will pop up asking to Save the current workspace: - Don’t save New Directory &gt; New Project In Directory name: write “epiproject_00” In Create project as a subdirectory of: - press browse, and navigate to a place on hyour computer you would like to store your R codes. I suggest this NOT TO BE your Downloads or Desktop folder. Create Project Now that we’re in a project, open a new R script. Save it in the project under the name “00_syntax” and let’s set started in the next chapter. "],["r-fundamentals.html", "Chapter 4 R fundamentals 4.1 Basic Datatypes in R 4.2 Special datatypes 4.3 Converting between datatypes 4.4 Data structures in R 4.5 Conditions and Comparisons 4.6 Conditional execution with 4.7 Repeating lines of code 4.8 Functions", " Chapter 4 R fundamentals Now that we know the basics of how this reader works, and how to organize our files in R, we can get starting with the basics of R. While the first few simple lines of code may not be very exciting, we will start with epidemiological applications soon! 4.1 Basic Datatypes in R When coding, we work with variables. Say we start our code with x=1, then x is a variable. We can do some calculations with x, and we can change x, depending on the type of data that the variable is. In R, variables can represent the following datatypes: Type Description Example Numeric Real number 3.14, -6.7 Integer Whole number 1, 4, 16 Character Text or string a, hello! Logical Boolean TRUE, FALSE The specification of a datatype is very important. While some datatypes may allow certain functionality, others may not. We can sum two numeric variables, which we can also do for two integers. When we try to add two characters to each other though, R will throw an error. Have a look yourself. summing two numeric datatypes x = 1.1 y = 2.9 print(x+y) ## [1] 4 summing two characters text1 = &#39;hello&#39; text2 = &#39;there!&#39; # print(text1+text2) -&gt; this will throw an error! While this doesn’t work, there are ways of combining, or rather concatenating two strings, namely using paste() and paste0(): paste(text1, text2) ## [1] &quot;hello there!&quot; paste0(text1, text2) ## [1] &quot;hellothere!&quot; Do you see the difference? paste() by default concatenates two strings with a space in between them, which paste0() does not do. 4.2 Special datatypes Besides the basic types of variables mentioned above, R also supports some special datatypes: Type Description Example Date Calendar date as.Date('2025-08-01') NA Missing value NULL Empty object (-) Inf Positive or negative infinity 1/0, -1/0 NaN Not a Number: undefined mathematics 0/0 When working with dates, it is very handy to convert what is normally read as text (so “2025-01-01”) into Date because there is some additional functionality for plotting dates, for example. The others, so NA, NULL, INF and NaN often show something is going wrong in the process, so watch out for these! 4.3 Converting between datatypes It is often necessary to convert data from one type to another. This process is called type conversion or casting. x &lt;- &quot;123&quot; num_x &lt;- as.numeric(x) print(num_x + 10) ## [1] 133 For conversion, watch out for these things: - NA introduction: if conversion fails, R returns an NA=&gt; as.numeric(\"hello\") - Loss of information: as.integer(3.99) will return 3, not 4! - Converting booleans: as.numeric(TRUE) returns 1 and as.numeric(FALSE) returns 0 4.4 Data structures in R While it’s nice to be able to work with variables and single data points, more often than not we want to work with objects that include multiple data points, and sometimes even different datatypes. These we call data structures. In R, we have the following ones: Structure Description Example List Collection of any datatypes list1 = ['A', 1, TRUE] Vector Sequence of elements of the same datatype vector1 = c(1,2,3)vector2 = c('A','B','C') Matrix 2-Dimensional - vector: one object of strictly the same datatype matrix1 = matrix(1:6, nrow=2) Array Multi-Dimensional matrix equivalent Factor A special type of vector: one with categorical values.Internally, these values are then seen as a limited number of distinct groups (called levels) vector_levels = c('high','medium','high')factor1 = factor(vector_levels) Data frame Table-like structure with columns that can be mixed datatypes.A single column has one specific datatype The Data frame is particularly important. Any excel table that we open in R is put into a Data frame. 4.4.1 Vectors: The Building Blocks Vectors are the most fundamental data structure in R. Here’s how to work with them: Creating vectors: # Numeric vector numbers &lt;- c(1, 2, 3, 4, 5) # Character vector names &lt;- c(&quot;Alice&quot;, &quot;Bob&quot;, &quot;Charlie&quot;) # Logical vector results &lt;- c(TRUE, FALSE, TRUE, TRUE) # Using sequences sequence1 &lt;- 1:10 # Numbers 1 to 10 sequence2 &lt;- seq(0, 1, 0.1) # From 0 to 1 in steps of 0.1 One of R’s powerful features is vectorization - operations apply to entire vectors at once: numbers &lt;- c(1, 2, 3, 4) numbers * 2 # Multiply all elements by 2: (2, 4, 6, 8) ## [1] 2 4 6 8 numbers + c(10, 20, 30, 40) # Add corresponding elements: (11, 22, 33, 44) ## [1] 11 22 33 44 4.4.2 Lists: Mixed Data Collections Lists are like vectors but can hold different datatypes. Think of them as containers where each “slot” can hold anything: # Creating a list with mixed datatypes mixed_list &lt;- list( name = &quot;John&quot;, age = 25, married = TRUE, children = c(&quot;Emma&quot;, &quot;Liam&quot;) ) # You can also create lists without names simple_list &lt;- list(&quot;text&quot;, 42, TRUE) When to use lists vs vectors Vectors: When all your data is the same type (all numbers, all text, etc.) Lists: When you need to store different types together, or when you want to store complex objects 4.4.3 Data Frames: The Excel of R Data frames are like Excel spreadsheets - they have rows and columns, where each column can be a different datatype but within a column, all values must be the same type. Creating a data frame: # Creating a simple data frame students &lt;- data.frame( name = c(&quot;Alice&quot;, &quot;Bob&quot;, &quot;Charlie&quot;), age = c(20, 21, 19), grade = c(&quot;A&quot;, &quot;B&quot;, &quot;A+&quot;), passed = c(TRUE, TRUE, TRUE) ) print(students) ## name age grade passed ## 1 Alice 20 A TRUE ## 2 Bob 21 B TRUE ## 3 Charlie 19 A+ TRUE 4.4.4 Indexing: Accessing Your Data Once you have data stored in these structures, you need to be able to get specific pieces out. This is called indexing or subsetting. Position-based indexing (using square brackets []): # indexing - vectors numbers_vector &lt;- c(10, 20, 30, 40, 50) numbers_vector[1] # First element: 10 ## [1] 10 numbers_vector[3] # Third element: 30 ## [1] 30 # you can also use a vector of indices numbers_vector[c(1, 3)] # First and third elements: 10, 30 ## [1] 10 30 # indexing - lists mixed_list &lt;- list(1, &quot;2&quot;, 6.7) mixed_list[1] # Returns a list containing just the name ## [[1]] ## [1] 1 mixed_list[2] ## [[1]] ## [1] &quot;2&quot; # indexing - data frame students &lt;- data.frame( name = c(&quot;Alice&quot;, &quot;Bob&quot;, &quot;Charlie&quot;), age = c(20, 21, 19), grade = c(&quot;A&quot;, &quot;B&quot;, &quot;A+&quot;) ) # Matrix-style indexing [row, column] students[1, 2] # First row, second column: 20 ## [1] 20 students[1, ] # Entire first row ## name age grade ## 1 Alice 20 A students[, 2] # Entire second column (ages) ## [1] 20 21 19 students[1:2, c(&quot;name&quot;, &quot;age&quot;)] # First two rows, name and age columns ## name age ## 1 Alice 20 ## 2 Bob 21 # List-style indexing (by column) students$name # The name column ## [1] &quot;Alice&quot; &quot;Bob&quot; &quot;Charlie&quot; students[[&quot;age&quot;]] # The age column ## [1] 20 21 19 students[&quot;grade&quot;] # The grade column (returns data frame) ## grade ## 1 A ## 2 B ## 3 A+ # Logical indexing students[students$age &gt; 20, ] # All rows where age &gt; 20 ## name age grade ## 2 Bob 21 B students[students$grade == &quot;A&quot;, ] # All rows where grade is &quot;A&quot; ## name age grade ## 1 Alice 20 A Understanding these data structures and how to index them is crucial because almost everything you do in R involves manipulating data stored in these formats. Whether you’re reading a CSV file (which becomes a data frame), performing calculations (often on vectors), or organizing complex results (in lists), these concepts form the foundation of effective R programming. 4.5 Conditions and Comparisons One thing we often need to do with our datatypes, are comparisons. This is basically checking whether a statement is TRUE or FALSE. Here are the basic comparison operators in R: Operator Meaning Example Result   == Equal to 3 == 3 TRUE != Not equal to 3 != 5 TRUE &gt; Greater than 4 &gt; 2 TRUE &lt; Less than 4 &lt; 2 FALSE &gt;= Greater than or equal to 4 &gt;= 4 TRUE &lt;= Less than or equal to 4 &lt;= 3 FALSE %in% Element inside an object 1 %in% c(1,2) TRUE And for combining logical statements, we use operators, of which these are available: Operator Meaning Example Result &amp; AND TRUE &amp; FALSE FALSE | OR TRUE | FALSE TRUE ! NOT !FALSE TRUE 4.6 Conditional execution with Sometimes we only want to execute a piece of code under a certain condition. For that, we have what is called an if-statement. In R we write if (...comparison...){code to be executed}. The basic if statement: x &lt;- 4 if (x &gt; 0) { print(&quot;x is positive&quot;) } ## [1] &quot;x is positive&quot; Besides the if part, we can also add an else part, which will be executed if the comparison is FALSE. if ... else x &lt;- -1 if (x &gt; 0) { print(&quot;x is positive&quot;) } else { print(&quot;x is not positive&quot;) } ## [1] &quot;x is not positive&quot; 4.7 Repeating lines of code Sometimes, we want to execute a piece of code a certain number of times, or we want to execute the same code based on a bunch of different variables. We can repeat code using a for - loop. Basically, we write for (iteration) {code to be executed}. For example, let’s print the numbers 1 to 5. For that to be done, first we write a vector from 1 to 5, which we do through 1:5. Then we iterate through all the elements in that vector by: for (i in 1:5) { # for i = 1 until i = 5 print(i) } ## [1] 1 ## [1] 2 ## [1] 3 ## [1] 4 ## [1] 5 Note that in the output, we see ## [1] numerous times. That’s because R prints something that only has one index every time. That was a classical example where we iterate through a bunch of integers. We can also iterate through other things: names = c(&#39;Marie Curie&#39;, &#39;Jenniffer Doudna&#39;, &#39;Alexander Flemming&#39;) for (name in names) { print(paste(name, &quot;won the Nobel Prize&quot;)) } ## [1] &quot;Marie Curie won the Nobel Prize&quot; ## [1] &quot;Jenniffer Doudna won the Nobel Prize&quot; ## [1] &quot;Alexander Flemming won the Nobel Prize&quot; 4.8 Functions People write code into functions in R. These are sometimes added into packages and published, so that anyone using R can use them. Examples of functions are paste(), log() (taking the logarithm base 10 of a number) and many others. When we make our own code, we sometimes end up writing huge chunks that we want to repeat, but can’t really put in a for loop. For that we can write our own functions! Here I’ll show an example testing whether or not our variable is negative or positive: positive_check &lt;- function(x) { # the name of the function is positive_or_negative, with the parameter &#39;x&#39; if (x &lt; 0) { # if x is smaller than 0 return(FALSE) # the output of the function is FALSE } else { # otherwise (if x is larger than, or equal to, 0) return(TRUE) # return TRUE } } And now that we’ve defined this function we can easily use it by positive_check(1), for example: positive_check(1) ## [1] TRUE positive_check(Inf) ## [1] TRUE "],["introduction-to-the-tidyverse.html", "Chapter 5 Introduction to the Tidyverse 5.1 Data Structure: Wide vs. Tidy Data 5.2 Core dplyr Functions for Data Manipulation 5.3 Additional Essential Functions 5.4 Practical Example: Data Analysis Workflow 5.5 Function Summary Table 5.6 Conclusion", " Chapter 5 Introduction to the Tidyverse The tidyverse is a collection of R packages designed for data science that share an underlying design philosophy, grammar, and data structures. These packages are essential tools for anyone working with data in R, providing intuitive and consistent functions for data cleaning, manipulation, and visualization. The tidyverse follows Hadley Wickham’s philosophy of tidy data, where: Each variable is a column Each observation is a row Each value is a cell This structure makes data analysis more intuitive and efficient. library(tidyverse) # Loads core tidyverse packages ## ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ── ## ✔ dplyr 1.1.4 ✔ readr 2.1.5 ## ✔ forcats 1.0.0 ✔ stringr 1.5.1 ## ✔ ggplot2 3.5.1 ✔ tibble 3.2.1 ## ✔ lubridate 1.9.3 ✔ tidyr 1.3.1 ## ✔ purrr 1.0.2 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() ## ℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors library(gapminder) # Example dataset 5.1 Data Structure: Wide vs. Tidy Data 5.1.1 Understanding the Difference Let’s explore the concept of tidy data with a practical example: # Create a wide-format dataset flu_cases &lt;- data.frame( year = c(2020, 2021), male_cases = c(120, 150), female_cases = c(100, 130) ) print(&quot;Wide format (not tidy):&quot;) ## [1] &quot;Wide format (not tidy):&quot; flu_cases ## year male_cases female_cases ## 1 2020 120 100 ## 2 2021 150 130 Why isn’t this tidy? The variable “sex” is encoded in the column names rather than being a proper column with values. 5.1.2 Reshaping Data with tidyr 5.1.2.1 Making Data Longer with pivot_longer() flu_tidy &lt;- flu_cases %&gt;% pivot_longer( cols = c(male_cases, female_cases), # Columns to pivot names_to = &quot;sex&quot;, # New column for variable names values_to = &quot;cases&quot; # New column for values ) %&gt;% mutate(sex = str_replace(sex, &quot;_cases&quot;, &quot;&quot;)) # Clean up the sex values print(&quot;Tidy format (long):&quot;) ## [1] &quot;Tidy format (long):&quot; flu_tidy ## # A tibble: 4 × 3 ## year sex cases ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 2020 male 120 ## 2 2020 female 100 ## 3 2021 male 150 ## 4 2021 female 130 5.1.2.2 Making Data Wider with pivot_wider() flu_wide_again &lt;- flu_tidy %&gt;% pivot_wider( names_from = sex, # Column to get new column names from values_from = cases, # Column to get values from names_glue = &quot;{sex}_cases&quot; # Template for new column names ) print(&quot;Back to wide format:&quot;) ## [1] &quot;Back to wide format:&quot; flu_wide_again ## # A tibble: 2 × 3 ## year male_cases female_cases ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2020 120 100 ## 2 2021 150 130 5.2 Core dplyr Functions for Data Manipulation Let’s explore the essential dplyr functions using the gapminder dataset. The Gapminder dataset is a widely used dataset in data science, statistics, and economics that provides a comprehensive view of global development over time. It contains information on countries’ life expectancy, GDP per capita, population, and other socio-economic indicators, often spanning several decades. The dataset is particularly popular for teaching and visualization because it allows users to explore trends across countries, continents, and time. For example, you can track how life expectancy has improved in different regions, how income inequality has changed, or how population growth interacts with economic development. Gapminder is also well known through the Gapminder Foundation and the animated bubble charts popularized by Hans Rosling, which make complex global trends intuitive and engaging. head(gapminder, 10) ## # A tibble: 10 × 6 ## country continent year lifeExp pop gdpPercap ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Afghanistan Asia 1952 28.8 8425333 779. ## 2 Afghanistan Asia 1957 30.3 9240934 821. ## 3 Afghanistan Asia 1962 32.0 10267083 853. ## 4 Afghanistan Asia 1967 34.0 11537966 836. ## 5 Afghanistan Asia 1972 36.1 13079460 740. ## 6 Afghanistan Asia 1977 38.4 14880372 786. ## 7 Afghanistan Asia 1982 39.9 12881816 978. ## 8 Afghanistan Asia 1987 40.8 13867957 852. ## 9 Afghanistan Asia 1992 41.7 16317921 649. ## 10 Afghanistan Asia 1997 41.8 22227415 635. 5.2.1 The Pipe Operator (%&gt;%) The pipe operator is the backbone of tidyverse workflows. It passes the result of one function as the first argument to the next function: # Instead of: function_c(function_b(function_a(data), arg1), arg2) # We write: data %&gt;% function_a() %&gt;% function_b(arg1) %&gt;% function_c(arg2) 5.2.2 select(): Choose Columns # Select specific columns selected_columns &lt;- gapminder %&gt;% select(country, year, lifeExp, pop) head(selected_columns) ## # A tibble: 6 × 4 ## country year lifeExp pop ## &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; ## 1 Afghanistan 1952 28.8 8425333 ## 2 Afghanistan 1957 30.3 9240934 ## 3 Afghanistan 1962 32.0 10267083 ## 4 Afghanistan 1967 34.0 11537966 ## 5 Afghanistan 1972 36.1 13079460 ## 6 Afghanistan 1977 38.4 14880372 # Select columns by pattern gapminder %&gt;% select(country, starts_with(&quot;life&quot;)) %&gt;% head() ## # A tibble: 6 × 2 ## country lifeExp ## &lt;fct&gt; &lt;dbl&gt; ## 1 Afghanistan 28.8 ## 2 Afghanistan 30.3 ## 3 Afghanistan 32.0 ## 4 Afghanistan 34.0 ## 5 Afghanistan 36.1 ## 6 Afghanistan 38.4 # Select everything except certain columns gapminder %&gt;% select(-continent, -gdpPercap) %&gt;% head() ## # A tibble: 6 × 4 ## country year lifeExp pop ## &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; ## 1 Afghanistan 1952 28.8 8425333 ## 2 Afghanistan 1957 30.3 9240934 ## 3 Afghanistan 1962 32.0 10267083 ## 4 Afghanistan 1967 34.0 11537966 ## 5 Afghanistan 1972 36.1 13079460 ## 6 Afghanistan 1977 38.4 14880372 5.2.3 filter(): Choose Rows # Filter by exact match gapminder %&gt;% filter(year == 2007) %&gt;% head() ## # A tibble: 6 × 6 ## country continent year lifeExp pop gdpPercap ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Afghanistan Asia 2007 43.8 31889923 975. ## 2 Albania Europe 2007 76.4 3600523 5937. ## 3 Algeria Africa 2007 72.3 33333216 6223. ## 4 Angola Africa 2007 42.7 12420476 4797. ## 5 Argentina Americas 2007 75.3 40301927 12779. ## 6 Australia Oceania 2007 81.2 20434176 34435. # Filter by multiple conditions gapminder %&gt;% filter(year == 2007, continent == &quot;Europe&quot;) %&gt;% head() ## # A tibble: 6 × 6 ## country continent year lifeExp pop gdpPercap ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Albania Europe 2007 76.4 3600523 5937. ## 2 Austria Europe 2007 79.8 8199783 36126. ## 3 Belgium Europe 2007 79.4 10392226 33693. ## 4 Bosnia and Herzegovina Europe 2007 74.9 4552198 7446. ## 5 Bulgaria Europe 2007 73.0 7322858 10681. ## 6 Croatia Europe 2007 75.7 4493312 14619. # Filter using %in% operator gapminder %&gt;% filter(continent %in% c(&#39;Oceania&#39;, &#39;Africa&#39;)) %&gt;% head() ## # A tibble: 6 × 6 ## country continent year lifeExp pop gdpPercap ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Algeria Africa 1952 43.1 9279525 2449. ## 2 Algeria Africa 1957 45.7 10270856 3014. ## 3 Algeria Africa 1962 48.3 11000948 2551. ## 4 Algeria Africa 1967 51.4 12760499 3247. ## 5 Algeria Africa 1972 54.5 14760787 4183. ## 6 Algeria Africa 1977 58.0 17152804 4910. # Filter by numeric conditions gapminder %&gt;% filter(pop &gt; 100000000, lifeExp &gt; 70) %&gt;% head() ## # A tibble: 6 × 6 ## country continent year lifeExp pop gdpPercap ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Brazil Americas 2002 71.0 179914212 8131. ## 2 Brazil Americas 2007 72.4 190010647 9066. ## 3 China Asia 1997 70.4 1230075000 2289. ## 4 China Asia 2002 72.0 1280400000 3119. ## 5 China Asia 2007 73.0 1318683096 4959. ## 6 Indonesia Asia 2007 70.6 223547000 3541. 5.2.4 mutate(): Create or Modify Columns # Create new columns gapminder_enhanced &lt;- gapminder %&gt;% mutate( pop_millions = pop / 1000000, gdp_total = pop * gdpPercap, pop_log = log10(pop) ) head(gapminder_enhanced) ## # A tibble: 6 × 9 ## country continent year lifeExp pop gdpPercap pop_millions gdp_total ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Afghanistan Asia 1952 28.8 8425333 779. 8.43 6.57e 9 ## 2 Afghanistan Asia 1957 30.3 9240934 821. 9.24 7.59e 9 ## 3 Afghanistan Asia 1962 32.0 10267083 853. 10.3 8.76e 9 ## 4 Afghanistan Asia 1967 34.0 11537966 836. 11.5 9.65e 9 ## 5 Afghanistan Asia 1972 36.1 13079460 740. 13.1 9.68e 9 ## 6 Afghanistan Asia 1977 38.4 14880372 786. 14.9 1.17e10 ## # ℹ 1 more variable: pop_log &lt;dbl&gt; # Conditional mutations with ifelse gapminder %&gt;% mutate( pop_category = ifelse(pop &gt; 50000000, &quot;Large&quot;, &quot;Small&quot;) ) %&gt;% head() ## # A tibble: 6 × 7 ## country continent year lifeExp pop gdpPercap pop_category ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Afghanistan Asia 1952 28.8 8425333 779. Small ## 2 Afghanistan Asia 1957 30.3 9240934 821. Small ## 3 Afghanistan Asia 1962 32.0 10267083 853. Small ## 4 Afghanistan Asia 1967 34.0 11537966 836. Small ## 5 Afghanistan Asia 1972 36.1 13079460 740. Small ## 6 Afghanistan Asia 1977 38.4 14880372 786. Small 5.2.5 transmute(): Create New Columns (Drop Others) # Keep only the newly created columns gapminder %&gt;% transmute( country = country, year = year, gdp_total = pop * gdpPercap, pop_millions = pop / 1000000 ) %&gt;% head() ## # A tibble: 6 × 4 ## country year gdp_total pop_millions ## &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Afghanistan 1952 6567086330. 8.43 ## 2 Afghanistan 1957 7585448670. 9.24 ## 3 Afghanistan 1962 8758855797. 10.3 ## 4 Afghanistan 1967 9648014150. 11.5 ## 5 Afghanistan 1972 9678553274. 13.1 ## 6 Afghanistan 1977 11697659231. 14.9 5.2.6 arrange(): Sort Rows # Sort by one column (ascending) gapminder %&gt;% filter(year == 2007) %&gt;% arrange(lifeExp) %&gt;% head() ## # A tibble: 6 × 6 ## country continent year lifeExp pop gdpPercap ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Swaziland Africa 2007 39.6 1133066 4513. ## 2 Mozambique Africa 2007 42.1 19951656 824. ## 3 Zambia Africa 2007 42.4 11746035 1271. ## 4 Sierra Leone Africa 2007 42.6 6144562 863. ## 5 Lesotho Africa 2007 42.6 2012649 1569. ## 6 Angola Africa 2007 42.7 12420476 4797. # Sort by multiple columns (descending) gapminder %&gt;% filter(year == 2007) %&gt;% arrange(desc(gdpPercap), desc(lifeExp)) %&gt;% head() ## # A tibble: 6 × 6 ## country continent year lifeExp pop gdpPercap ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Norway Europe 2007 80.2 4627926 49357. ## 2 Kuwait Asia 2007 77.6 2505559 47307. ## 3 Singapore Asia 2007 80.0 4553009 47143. ## 4 United States Americas 2007 78.2 301139947 42952. ## 5 Ireland Europe 2007 78.9 4109086 40676. ## 6 Hong Kong, China Asia 2007 82.2 6980412 39725. 5.2.7 group_by() and summarise(): Group Operations # Basic grouping and summarizing gapminder %&gt;% filter(year == 2007) %&gt;% group_by(continent) %&gt;% summarise( mean_lifeExp = mean(lifeExp), sd_lifeExp = sd(lifeExp), total_pop = sum(pop), n_countries = n(), .groups = &#39;drop&#39; # Automatically ungroup ) ## # A tibble: 5 × 5 ## continent mean_lifeExp sd_lifeExp total_pop n_countries ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 Africa 54.8 9.63 929539692 52 ## 2 Americas 73.6 4.44 898871184 25 ## 3 Asia 70.7 7.96 3811953827 33 ## 4 Europe 77.6 2.98 586098529 30 ## 5 Oceania 80.7 0.729 24549947 2 # Multiple grouping variables gapminder %&gt;% filter(year %in% c(1997, 2007)) %&gt;% group_by(continent, year) %&gt;% summarise( avg_gdp = mean(gdpPercap), countries = n(), .groups = &#39;drop&#39; ) ## # A tibble: 10 × 4 ## continent year avg_gdp countries ## &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; ## 1 Africa 1997 2379. 52 ## 2 Africa 2007 3089. 52 ## 3 Americas 1997 8889. 25 ## 4 Americas 2007 11003. 25 ## 5 Asia 1997 9834. 33 ## 6 Asia 2007 12473. 33 ## 7 Europe 1997 19077. 30 ## 8 Europe 2007 25054. 30 ## 9 Oceania 1997 24024. 2 ## 10 Oceania 2007 29810. 2 5.3 Additional Essential Functions 5.3.1 rename(): Change Column Names # Rename columns gapminder %&gt;% rename( life_expectancy = lifeExp, gdp_per_capita = gdpPercap, population = pop ) %&gt;% head() ## # A tibble: 6 × 6 ## country continent year life_expectancy population gdp_per_capita ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Afghanistan Asia 1952 28.8 8425333 779. ## 2 Afghanistan Asia 1957 30.3 9240934 821. ## 3 Afghanistan Asia 1962 32.0 10267083 853. ## 4 Afghanistan Asia 1967 34.0 11537966 836. ## 5 Afghanistan Asia 1972 36.1 13079460 740. ## 6 Afghanistan Asia 1977 38.4 14880372 786. # Rename with a function gapminder %&gt;% rename_with(toupper, c(country, continent)) %&gt;% head() ## # A tibble: 6 × 6 ## COUNTRY CONTINENT year lifeExp pop gdpPercap ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Afghanistan Asia 1952 28.8 8425333 779. ## 2 Afghanistan Asia 1957 30.3 9240934 821. ## 3 Afghanistan Asia 1962 32.0 10267083 853. ## 4 Afghanistan Asia 1967 34.0 11537966 836. ## 5 Afghanistan Asia 1972 36.1 13079460 740. ## 6 Afghanistan Asia 1977 38.4 14880372 786. 5.3.2 Data Type Conversions # Create sample data with mixed types sample_data &lt;- data.frame( id = c(&quot;1&quot;, &quot;2&quot;, &quot;3&quot;), date_string = c(&quot;2020-01-01&quot;, &quot;2020-02-01&quot;, &quot;2020-03-01&quot;), value = c(&quot;10.5&quot;, &quot;20.3&quot;, &quot;15.7&quot;), stringsAsFactors = FALSE ) # Convert data types sample_data_converted &lt;- sample_data %&gt;% mutate( id = as.numeric(id), date_string = as.Date(date_string), value = as.numeric(value), id_char = as.character(id) ) str(sample_data_converted) ## &#39;data.frame&#39;: 3 obs. of 4 variables: ## $ id : num 1 2 3 ## $ date_string: Date, format: &quot;2020-01-01&quot; &quot;2020-02-01&quot; ... ## $ value : num 10.5 20.3 15.7 ## $ id_char : chr &quot;1&quot; &quot;2&quot; &quot;3&quot; 5.3.3 across(): Apply Functions to Multiple Columns # Apply function to multiple columns gapminder %&gt;% group_by(continent) %&gt;% summarise( across(c(lifeExp, pop, gdpPercap), mean), .groups = &#39;drop&#39; ) ## # A tibble: 5 × 4 ## continent lifeExp pop gdpPercap ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Africa 48.9 9916003. 2194. ## 2 Americas 64.7 24504795. 7136. ## 3 Asia 60.1 77038722. 7902. ## 4 Europe 71.9 17169765. 14469. ## 5 Oceania 74.3 8874672. 18622. # Apply multiple functions gapminder %&gt;% filter(year == 2007) %&gt;% group_by(continent) %&gt;% summarise( across(c(lifeExp, gdpPercap), list(mean = mean, sd = sd, min = min, max = max)), .groups = &#39;drop&#39; ) ## # A tibble: 5 × 9 ## continent lifeExp_mean lifeExp_sd lifeExp_min lifeExp_max gdpPercap_mean ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Africa 54.8 9.63 39.6 76.4 3089. ## 2 Americas 73.6 4.44 60.9 80.7 11003. ## 3 Asia 70.7 7.96 43.8 82.6 12473. ## 4 Europe 77.6 2.98 71.8 81.8 25054. ## 5 Oceania 80.7 0.729 80.2 81.2 29810. ## # ℹ 3 more variables: gdpPercap_sd &lt;dbl&gt;, gdpPercap_min &lt;dbl&gt;, ## # gdpPercap_max &lt;dbl&gt; 5.3.4 recode(): Recode Values # Recode values gapminder %&gt;% mutate( continent_short = recode(continent, &quot;Africa&quot; = &quot;AF&quot;, &quot;Americas&quot; = &quot;AM&quot;, &quot;Asia&quot; = &quot;AS&quot;, &quot;Europe&quot; = &quot;EU&quot;, &quot;Oceania&quot; = &quot;OC&quot; ) ) %&gt;% select(country, continent, continent_short) %&gt;% head() ## # A tibble: 6 × 3 ## country continent continent_short ## &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; ## 1 Afghanistan Asia AS ## 2 Afghanistan Asia AS ## 3 Afghanistan Asia AS ## 4 Afghanistan Asia AS ## 5 Afghanistan Asia AS ## 6 Afghanistan Asia AS 5.3.5 case_when(): Multiple Conditional Logic # Multiple conditions with case_when gapminder %&gt;% mutate( development_level = case_when( gdpPercap &lt; 1000 ~ &quot;Low income&quot;, gdpPercap &lt; 5000 ~ &quot;Lower middle income&quot;, gdpPercap &lt; 15000 ~ &quot;Upper middle income&quot;, gdpPercap &gt;= 15000 ~ &quot;High income&quot;, TRUE ~ &quot;Unknown&quot; # Default case ), pop_size = case_when( pop &lt; 10000000 ~ &quot;Small&quot;, pop &lt; 50000000 ~ &quot;Medium&quot;, pop &gt;= 50000000 ~ &quot;Large&quot; ) ) %&gt;% select(country, year, gdpPercap, development_level, pop, pop_size) %&gt;% filter(year == 2007) %&gt;% head(10) ## # A tibble: 10 × 6 ## country year gdpPercap development_level pop pop_size ## &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; ## 1 Afghanistan 2007 975. Low income 31889923 Medium ## 2 Albania 2007 5937. Upper middle income 3600523 Small ## 3 Algeria 2007 6223. Upper middle income 33333216 Medium ## 4 Angola 2007 4797. Lower middle income 12420476 Medium ## 5 Argentina 2007 12779. Upper middle income 40301927 Medium ## 6 Australia 2007 34435. High income 20434176 Medium ## 7 Austria 2007 36126. High income 8199783 Small ## 8 Bahrain 2007 29796. High income 708573 Small ## 9 Bangladesh 2007 1391. Lower middle income 150448339 Large ## 10 Belgium 2007 33693. High income 10392226 Medium 5.3.6 replace_na(): Handle Missing Values # Create data with missing values data_with_na &lt;- data.frame( name = c(&quot;Alice&quot;, &quot;Bob&quot;, NA, &quot;David&quot;), age = c(25, NA, 35, 40), city = c(&quot;New York&quot;, &quot;Boston&quot;, &quot;Chicago&quot;, NA) ) print(&quot;Original data:&quot;) ## [1] &quot;Original data:&quot; data_with_na ## name age city ## 1 Alice 25 New York ## 2 Bob NA Boston ## 3 &lt;NA&gt; 35 Chicago ## 4 David 40 &lt;NA&gt; # Replace NA values data_cleaned &lt;- data_with_na %&gt;% mutate( name = replace_na(name, &quot;Unknown&quot;), age = replace_na(age, median(age, na.rm = TRUE)), city = replace_na(city, &quot;Not specified&quot;) ) print(&quot;After replacing NA values:&quot;) ## [1] &quot;After replacing NA values:&quot; data_cleaned ## name age city ## 1 Alice 25 New York ## 2 Bob 35 Boston ## 3 Unknown 35 Chicago ## 4 David 40 Not specified 5.3.7 which(): Find Positions # Using which to find positions sample_vector &lt;- c(10, 25, 30, 15, 40, 35) # Find positions where condition is TRUE positions &lt;- which(sample_vector &gt; 20) print(paste(&quot;Positions where value &gt; 20:&quot;, paste(positions, collapse = &quot;, &quot;))) ## [1] &quot;Positions where value &gt; 20: 2, 3, 5, 6&quot; # Use in data frame context gapminder %&gt;% filter(year == 2007) %&gt;% mutate(rank_gdp = rank(desc(gdpPercap))) %&gt;% filter(rank_gdp &lt;= 5) %&gt;% select(country, gdpPercap, rank_gdp) %&gt;% arrange(rank_gdp) ## # A tibble: 5 × 3 ## country gdpPercap rank_gdp ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Norway 49357. 1 ## 2 Kuwait 47307. 2 ## 3 Singapore 47143. 3 ## 4 United States 42952. 4 ## 5 Ireland 40676. 5 5.4 Practical Example: Data Analysis Workflow This example analyzes life expectancy patterns across continents, decades, and economic development levels using the gapminder dataset. The goal is to uncover how life expectancy varies when we consider geographic location, time period (1990s vs 2000s), and economic status (GDP per capita categories) simultaneously. The result is a structured summary showing average life expectancy for different combinations of these factors, filtered to ensure statistical reliability. workflow create categorical variables for time periods and GDP levels group by continent, decade, and GDP category calculate summary statistics remove unreliable small groups arrange results for easy interpretation. # Comprehensive analysis: Life expectancy trends by continent life_exp_analysis &lt;- gapminder %&gt;% # Filter for recent decades filter(year &gt;= 1990) %&gt;% # Create additional variables mutate( decade = case_when( year %in% 1990:1999 ~ &quot;1990s&quot;, year %in% 2000:2007 ~ &quot;2000s&quot; ), gdp_category = case_when( gdpPercap &lt; 2000 ~ &quot;Low GDP&quot;, gdpPercap &lt; 10000 ~ &quot;Medium GDP&quot;, gdpPercap &gt;= 10000 ~ &quot;High GDP&quot; ) ) %&gt;% # Group and summarize group_by(continent, decade, gdp_category) %&gt;% summarise( avg_life_exp = mean(lifeExp), countries = n(), total_pop = sum(pop), .groups = &#39;drop&#39; ) %&gt;% # Filter out small groups filter(countries &gt;= 3) %&gt;% # Arrange results arrange(continent, decade, desc(avg_life_exp)) print(&quot;Life expectancy analysis by continent, decade, and GDP category:&quot;) ## [1] &quot;Life expectancy analysis by continent, decade, and GDP category:&quot; life_exp_analysis ## # A tibble: 21 × 6 ## continent decade gdp_category avg_life_exp countries total_pop ## &lt;fct&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Africa 1990s Medium GDP 61.9 28 381335877 ## 2 Africa 1990s Low GDP 50.3 74 1019466696 ## 3 Africa 2000s Medium GDP 59.7 27 632430409 ## 4 Africa 2000s High GDP 58.5 7 13862646 ## 5 Africa 2000s Low GDP 51.5 70 1116970553 ## 6 Americas 1990s High GDP 75.1 10 689423253 ## 7 Americas 1990s Medium GDP 69.9 38 833511034 ## 8 Americas 2000s High GDP 76.3 15 976865109 ## 9 Americas 2000s Medium GDP 72.3 33 755668372 ## 10 Asia 1990s High GDP 75.0 21 479574324 ## # ℹ 11 more rows 5.5 Function Summary Table Here’s a comprehensive summary of all the tidyverse functions covered: Function Description Example select() Choose specific columns from a dataset select(country, year, pop) filter() Choose specific rows based on conditions filter(year == 2007, pop &gt; 1000000) mutate() Create new columns or modify existing ones mutate(gdp_total = pop * gdpPercap) transmute() Create new columns and drop all others transmute(country, gdp_billions = gdp/1e9) arrange() Sort rows by one or more columns arrange(desc(lifeExp)) group_by() Group rows for grouped operations group_by(continent, year) summarise() Calculate summary statistics for groups summarise(mean_life = mean(lifeExp)) rename() Change column names rename(life_expectancy = lifeExp) pivot_longer() Convert wide data to long format (tidy) pivot_longer(cols = c(col1, col2)) pivot_wider() Convert long data to wide format pivot_wider(names_from = key) across() Apply functions to multiple columns at once across(c(col1, col2), mean) recode() Recode values in a column recode(continent, 'Asia' = 'AS') case_when() Handle multiple conditional logic case_when(pop &lt; 1e6 ~ 'Small') replace_na() Replace missing (NA) values replace_na(column, 'Unknown') as.numeric() Convert to numeric data type as.numeric(character_column) as.character() Convert to character data type as.character(numeric_column) which() Find positions where condition is TRUE which(vector &gt; threshold) 5.6 Conclusion The tidyverse provides a consistent and intuitive grammar for data manipulation in R. By mastering these core functions, you’ll be able to efficiently clean, transform, and analyze data. The key principles to remember are: Start with tidy data - each variable in a column, each observation in a row Use the pipe operator (%&gt;%) to chain operations together Combine functions to create powerful data manipulation workflows Group operations allow for sophisticated analyses across different categories "],["data-visualization-in-r.html", "Chapter 6 Data Visualization in R 6.1 ggplot2: The Grammar of Graphics 6.2 Basic Plot Types 6.3 Aesthetic Mappings 6.4 Faceting: Small Multiples 6.5 Statistical Transformations 6.6 Scales: Controlling Aesthetic Mappings 6.7 Coordinates and Transformations 6.8 Labels and Themes 6.9 Heatmaps 6.10 Combining Multiple Geoms 6.11 Text and Annotations 6.12 Practical Example: Complex Multi-layered Plot 6.13 ggplot2 Function Summary Table 6.14 Tips and Tricks 6.15 Conclusion 6.16 My favorite figures I ever made", " Chapter 6 Data Visualization in R The tidyverse is a bit scary perhaps, when only getting into it. It is very quick to learn though, I promise. The best thing for me personally about the tidyverse, and perhaps about all of R, are the figures one can make with ggplot2. It really does not take that much to produce beautiful figures, that are basically publication-ready. It is not that difficult and the figures are much more beautiful than one could ever make with Excel or Statica. 6.1 ggplot2: The Grammar of Graphics ggplot2 is the tidyverse package for data visualization, based on Leland Wilkinson’s “Grammar of Graphics.” Unlike base R plotting, ggplot2 builds plots layer by layer using a consistent grammar that makes complex visualizations intuitive and customizable. The grammar of graphics breaks down any plot into fundamental components: Data: The dataset you want to visualize Aesthetics (aes): How variables map to visual properties (x, y, color, size, etc.) Geometries (geom): The visual elements that represent data (points, lines, bars, etc.) Statistics (stat): Statistical transformations of the data (counts, means, etc.) Scales: How aesthetic mappings translate to visual values Coordinate systems: How data maps to the plot area Themes: Overall visual appearance 6.1.1 Basic ggplot2 Structure Every ggplot follows this basic template: # Basic template ggplot(data = &lt;DATA&gt;) + &lt;GEOM_FUNCTION&gt;(mapping = aes(&lt;MAPPINGS&gt;)) + &lt;OTHER_LAYERS&gt; Let’s start with the gapminder dataset to explore these concepts: library(ggplot2) library(gapminder) library(tidyverse) ## ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ── ## ✔ dplyr 1.1.4 ✔ readr 2.1.5 ## ✔ forcats 1.0.0 ✔ stringr 1.5.1 ## ✔ lubridate 1.9.3 ✔ tibble 3.2.1 ## ✔ purrr 1.0.2 ✔ tidyr 1.3.1 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() ## ℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors # Preview the data head(gapminder) ## # A tibble: 6 × 6 ## country continent year lifeExp pop gdpPercap ## &lt;fct&gt; &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Afghanistan Asia 1952 28.8 8425333 779. ## 2 Afghanistan Asia 1957 30.3 9240934 821. ## 3 Afghanistan Asia 1962 32.0 10267083 853. ## 4 Afghanistan Asia 1967 34.0 11537966 836. ## 5 Afghanistan Asia 1972 36.1 13079460 740. ## 6 Afghanistan Asia 1977 38.4 14880372 786. 6.2 Basic Plot Types 6.2.1 Scatter Plots with geom_point() # Basic scatter plot ggplot(data = gapminder, mapping = aes(x = gdpPercap, y = lifeExp)) + geom_point() # Add color mapping ggplot(data = gapminder, aes(x = gdpPercap, y = lifeExp, color = continent)) + geom_point() # Add size mapping ggplot(data = gapminder, aes(x = gdpPercap, y = lifeExp, color = continent, size = pop)) + geom_point(alpha = 0.7) # alpha controls transparency 6.2.2 Line Plots with geom_line() # Line plot showing trends over time gapminder %&gt;% filter(country %in% c(&quot;United States&quot;, &quot;China&quot;, &quot;India&quot;, &quot;Germany&quot;)) %&gt;% ggplot(aes(x = year, y = lifeExp, color = country)) + geom_line(size = 1.2) ## Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0. ## ℹ Please use `linewidth` instead. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was ## generated. # Multiple lines with points gapminder %&gt;% filter(continent == &quot;Oceania&quot;) %&gt;% ggplot(aes(x = year, y = gdpPercap, color = country)) + geom_line() + geom_point() 6.2.3 Bar Plots with geom_col() and geom_bar() # geom_col: heights of bars represent values in data gapminder %&gt;% filter(year == 2007, continent == &quot;Americas&quot;) %&gt;% ggplot(aes(x = country, y = pop)) + geom_col() + coord_flip() # Flip coordinates for better readability # geom_bar: counts observations gapminder %&gt;% filter(year == 2007) %&gt;% ggplot(aes(x = continent)) + geom_bar() # Stacked bar chart gapminder %&gt;% filter(year %in% c(1997, 2007)) %&gt;% mutate(year = as.factor(year)) %&gt;% ggplot(aes(x = continent, fill = year)) + geom_bar(position = &quot;stack&quot;) 6.2.4 Histograms with geom_histogram() # Basic histogram gapminder %&gt;% filter(year == 2007) %&gt;% ggplot(aes(x = lifeExp)) + geom_histogram(bins = 20, fill = &quot;skyblue&quot;, color = &quot;black&quot;) # Histogram with different binwidths gapminder %&gt;% filter(year == 2007) %&gt;% ggplot(aes(x = gdpPercap)) + geom_histogram(binwidth = 5000, fill = &quot;lightgreen&quot;, alpha = 0.7) 6.2.5 Box Plots with geom_boxplot() # Basic box plot gapminder %&gt;% filter(year == 2007) %&gt;% ggplot(aes(x = continent, y = lifeExp)) + geom_boxplot() # Box plot with jittered points gapminder %&gt;% filter(year == 2007) %&gt;% ggplot(aes(x = continent, y = lifeExp, fill = continent)) + geom_boxplot(alpha = 0.7) + geom_jitter(width = 0.2, alpha = 0.5) 6.3 Aesthetic Mappings 6.3.1 Understanding aes() Aesthetics map variables to visual properties. You can set them globally (in ggplot()) or locally (in individual geom functions): # Global aesthetic mappings gapminder %&gt;% filter(year == 2007) %&gt;% ggplot(aes(x = gdpPercap, y = lifeExp, color = continent, size = pop)) + geom_point(alpha = 0.7) # Local aesthetic mappings gapminder %&gt;% filter(year == 2007) %&gt;% ggplot(aes(x = gdpPercap, y = lifeExp)) + geom_point(aes(color = continent, size = pop), alpha = 0.7) 6.3.2 Fixed vs. Mapped Aesthetics # WRONG: color inside aes() when you want a fixed color # This creates a legend with one color ggplot(gapminder, aes(x = gdpPercap, y = lifeExp, color = &quot;blue&quot;)) + geom_point() # CORRECT: fixed color outside aes() ggplot(gapminder, aes(x = gdpPercap, y = lifeExp)) + geom_point(color = &quot;blue&quot;) # CORRECT: mapped color inside aes() ggplot(gapminder, aes(x = gdpPercap, y = lifeExp, color = continent)) + geom_point() 6.4 Faceting: Small Multiples 6.4.1 facet_wrap() # Facet by one variable gapminder %&gt;% filter(year %in% c(1977, 1987, 1997, 2007)) %&gt;% ggplot(aes(x = gdpPercap, y = lifeExp, color = continent)) + geom_point() + facet_wrap(~ year) # Control facet layout gapminder %&gt;% ggplot(aes(x = year, y = lifeExp, group = country)) + geom_line(alpha = 0.3) + facet_wrap(~ continent, nrow = 2) 6.4.2 facet_grid() # Facet by two variables gapminder %&gt;% filter(year %in% c(1997, 2007), continent %in% c(&quot;Americas&quot;, &quot;Europe&quot;, &quot;Asia&quot;)) %&gt;% mutate(pop_category = ifelse(pop &gt; 50000000, &quot;Large&quot;, &quot;Small&quot;)) %&gt;% ggplot(aes(x = gdpPercap, y = lifeExp, color = continent)) + geom_point() + facet_grid(pop_category ~ year) 6.5 Statistical Transformations 6.5.1 Smooth Lines with geom_smooth() # Add trend line gapminder %&gt;% filter(year == 2007) %&gt;% ggplot(aes(x = gdpPercap, y = lifeExp)) + geom_point(aes(color = continent), alpha = 0.7) + geom_smooth(method = &quot;lm&quot;, se = FALSE, color = &quot;black&quot;) ## `geom_smooth()` using formula = &#39;y ~ x&#39; # Separate trend lines by group gapminder %&gt;% filter(year == 2007) %&gt;% ggplot(aes(x = gdpPercap, y = lifeExp, color = continent)) + geom_point(alpha = 0.7) + geom_smooth(method = &quot;lm&quot;, se = FALSE) ## `geom_smooth()` using formula = &#39;y ~ x&#39; 6.5.2 Statistical Summaries # stat_summary for custom statistics gapminder %&gt;% filter(year == 2007) %&gt;% ggplot(aes(x = continent, y = lifeExp)) + stat_summary(fun = mean, geom = &quot;point&quot;, size = 3, color = &quot;red&quot;) + stat_summary(fun.data = mean_se, geom = &quot;errorbar&quot;, width = 0.2) + geom_jitter(alpha = 0.3, width = 0.2) 6.6 Scales: Controlling Aesthetic Mappings 6.6.1 Color Scales # Manual color scale gapminder %&gt;% filter(year == 2007) %&gt;% ggplot(aes(x = gdpPercap, y = lifeExp, color = continent)) + geom_point(size = 2) + scale_color_manual(values = c(&quot;Africa&quot; = &quot;red&quot;, &quot;Americas&quot; = &quot;blue&quot;, &quot;Asia&quot; = &quot;green&quot;, &quot;Europe&quot; = &quot;orange&quot;, &quot;Oceania&quot; = &quot;purple&quot;)) # Brewer color palette gapminder %&gt;% filter(year == 2007) %&gt;% ggplot(aes(x = gdpPercap, y = lifeExp, color = continent)) + geom_point(size = 2) + scale_color_brewer(type = &quot;qual&quot;, palette = &quot;Set2&quot;) # Viridis color palette gapminder %&gt;% filter(year == 2007) %&gt;% ggplot(aes(x = gdpPercap, y = lifeExp, color = pop)) + geom_point(size = 2) + scale_color_viridis_c() 6.6.2 Axis Scales # Log scale for x-axis gapminder %&gt;% filter(year == 2007) %&gt;% ggplot(aes(x = gdpPercap, y = lifeExp, color = continent)) + geom_point(size = 2) + scale_x_log10() # Custom breaks and labels gapminder %&gt;% filter(year == 2007) %&gt;% ggplot(aes(x = gdpPercap, y = lifeExp, color = continent)) + geom_point(size = 2) + scale_x_continuous(breaks = seq(0, 50000, 10000), labels = paste0(&quot;$&quot;, seq(0, 50, 10), &quot;K&quot;)) # Limit axis ranges gapminder %&gt;% filter(year == 2007) %&gt;% ggplot(aes(x = gdpPercap, y = lifeExp, color = continent)) + geom_point(size = 2) + xlim(0, 30000) + ylim(40, 85) ## Warning: Removed 21 rows containing missing values or values outside the scale range ## (`geom_point()`). 6.7 Coordinates and Transformations # Flip coordinates gapminder %&gt;% filter(year == 2007, continent == &quot;Europe&quot;) %&gt;% ggplot(aes(x = reorder(country, lifeExp), y = lifeExp)) + geom_col() + coord_flip() # Polar coordinates (for pie charts) gapminder %&gt;% filter(year == 2007) %&gt;% count(continent) %&gt;% ggplot(aes(x = &quot;&quot;, y = n, fill = continent)) + geom_col() + coord_polar(theta = &quot;y&quot;) # Fixed aspect ratio gapminder %&gt;% filter(year == 2007) %&gt;% ggplot(aes(x = gdpPercap, y = lifeExp)) + geom_point() + coord_fixed(ratio = 300) # 300 GDP per capita = 1 year life expectancy 6.8 Labels and Themes 6.8.1 Adding Labels # Basic labels gapminder %&gt;% filter(year == 2007) %&gt;% ggplot(aes(x = gdpPercap, y = lifeExp, color = continent)) + geom_point(size = 2) + labs( title = &quot;Life Expectancy vs GDP Per Capita (2007)&quot;, subtitle = &quot;Each point represents a country&quot;, x = &quot;GDP per capita (USD)&quot;, y = &quot;Life expectancy (years)&quot;, color = &quot;Continent&quot;, caption = &quot;Data source: Gapminder&quot; ) 6.8.2 Theme Customization # Built-in themes p &lt;- gapminder %&gt;% filter(year == 2007) %&gt;% ggplot(aes(x = gdpPercap, y = lifeExp, color = continent)) + geom_point(size = 2) # Different theme options p + theme_minimal() p + theme_classic() p + theme_dark() p + theme_void() # Custom theme modifications p + theme_minimal() + theme( plot.title = element_text(size = 16, face = &quot;bold&quot;), axis.text = element_text(size = 12), legend.position = &quot;bottom&quot;, panel.grid.minor = element_blank() ) 6.9 Heatmaps Coming soon … 6.10 Combining Multiple Geoms # Combining points and lines gapminder %&gt;% filter(continent == &quot;Americas&quot;) %&gt;% group_by(year) %&gt;% summarise(avg_life_exp = mean(lifeExp), .groups = &#39;drop&#39;) %&gt;% ggplot(aes(x = year, y = avg_life_exp)) + geom_line(size = 1.2, color = &quot;blue&quot;) + geom_point(size = 3, color = &quot;red&quot;) + labs(title = &quot;Average Life Expectancy in Americas Over Time&quot;, y = &quot;Average Life Expectancy&quot;) # Error bars with points gapminder %&gt;% filter(year == 2007) %&gt;% group_by(continent) %&gt;% summarise( mean_life = mean(lifeExp), se_life = sd(lifeExp) / sqrt(n()), .groups = &#39;drop&#39; ) %&gt;% ggplot(aes(x = continent, y = mean_life)) + geom_col(fill = &quot;lightblue&quot;, alpha = 0.7) + geom_errorbar(aes(ymin = mean_life - se_life, ymax = mean_life + se_life), width = 0.2) 6.11 Text and Annotations # Add text labels to points gapminder %&gt;% filter(year == 2007, continent == &quot;Europe&quot;) %&gt;% ggplot(aes(x = gdpPercap, y = lifeExp)) + geom_point() + geom_text(aes(label = country), size = 3, hjust = -0.1) # Better: use geom_text_repel to avoid overlapping library(ggrepel) gapminder %&gt;% filter(year == 2007, continent == &quot;Europe&quot;) %&gt;% ggplot(aes(x = gdpPercap, y = lifeExp)) + geom_point() + geom_text_repel(aes(label = country), size = 3) 6.12 Practical Example: Complex Multi-layered Plot # Complex visualization combining multiple elements complex_plot &lt;- gapminder %&gt;% filter(year %in% c(1977, 1987, 1997, 2007)) %&gt;% ggplot(aes(x = gdpPercap, y = lifeExp)) + # Add points with continent color and population size geom_point(aes(color = continent, size = pop), alpha = 0.7) + # Add trend line geom_smooth(method = &quot;lm&quot;, se = FALSE, color = &quot;black&quot;, size = 0.5) + # Facet by year facet_wrap(~ year, nrow = 2) + # Use log scale for x-axis scale_x_log10(breaks = c(500, 2000, 8000, 32000), labels = c(&quot;$500&quot;, &quot;$2,000&quot;, &quot;$8,000&quot;, &quot;$32,000&quot;)) + # Customize colors scale_color_brewer(type = &quot;qual&quot;, palette = &quot;Set2&quot;) + # Adjust size scale scale_size_continuous(range = c(1, 8), guide = &quot;none&quot;) + # Add labels labs( title = &quot;The Relationship Between Wealth and Health Over Time&quot;, subtitle = &quot;GDP per capita vs Life Expectancy (1977-2007)&quot;, x = &quot;GDP per capita (log scale)&quot;, y = &quot;Life expectancy (years)&quot;, color = &quot;Continent&quot;, caption = &quot;Point size represents population size. Data: Gapminder&quot; ) + # Customize theme theme_minimal() + theme( plot.title = element_text(size = 14, face = &quot;bold&quot;), plot.subtitle = element_text(size = 11), strip.text = element_text(size = 10, face = &quot;bold&quot;), legend.position = &quot;bottom&quot; ) complex_plot ## `geom_smooth()` using formula = &#39;y ~ x&#39; 6.13 ggplot2 Function Summary Table Function Category Description Example ggplot() Core Initialize a plot with data and aesthetics ggplot(data, aes(x = var1, y = var2)) aes() Core Define aesthetic mappings aes(x = gdp, y = life, color = continent) geom_point() Geoms Add points (scatter plot) geom_point(size = 2, alpha = 0.7) geom_line() Geoms Add lines geom_line(size = 1.2) geom_col() Geoms Add bars with heights from data geom_col(fill = \"blue\") geom_bar() Geoms Add bars that count observations geom_bar() geom_histogram() Geoms Add histogram geom_histogram(bins = 20) geom_boxplot() Geoms Add box plot geom_boxplot() geom_density() Geoms Add density curve geom_density(alpha = 0.5) geom_violin() Geoms Add violin plot geom_violin() geom_tile() Geoms Add tiles (heatmap) geom_tile() geom_smooth() Geoms Add smoothed trend line geom_smooth(method = \"lm\") geom_text() Geoms Add text labels geom_text(aes(label = country)) geom_jitter() Geoms Add jittered points geom_jitter(width = 0.2) facet_wrap() Facets Create subplots in a wrapped layout facet_wrap(~ variable) facet_grid() Facets Create subplots in a grid facet_grid(var1 ~ var2) scale_x_continuous() Scales Customize continuous x-axis scale_x_continuous(breaks = c(1,2,3)) scale_x_log10() Scales Use log10 scale for x-axis scale_x_log10() scale_color_manual() Scales Manually set colors scale_color_manual(values = c(\"red\", \"blue\")) scale_color_brewer() Scales Use ColorBrewer palette scale_color_brewer(palette = \"Set2\") scale_color_viridis_c() Scales Use Viridis color scale (continuous) scale_color_viridis_c() xlim() / ylim() Scales Set axis limits xlim(0, 100) coord_flip() Coordinates Flip x and y coordinates coord_flip() coord_polar() Coordinates Use polar coordinates coord_polar(theta = \"y\") coord_fixed() Coordinates Fix aspect ratio coord_fixed(ratio = 1) labs() Labels Add/modify plot labels labs(title = \"My Plot\", x = \"X axis\") ggtitle() Labels Add plot title ggtitle(\"My Plot Title\") xlab() / ylab() Labels Add axis labels xlab(\"X axis label\") theme() Themes Customize plot appearance theme(legend.position = \"bottom\") theme_minimal() Themes Apply minimal theme theme_minimal() theme_classic() Themes Apply classic theme theme_classic() theme_dark() Themes Apply dark theme theme_dark() annotate() Annotations Add annotations to plot annotate(\"text\", x = 1, y = 2, label = \"Note\") 6.14 Tips and Tricks Colors A good colorscale can make an enormous difference. For discrete data I like to use the scales Paired, Dark2 and for continuous data I like using the inferno, magma and coolwarm palettes. Make sure the scale you choose is intuitive and easy to interpret. When making figures ready for shared or published work, make sure that it is colorblind-friendly (the viridis color palettes are especially chosen for this). And when preparing for a presentation always make sure that your figure would still be readable if the resolution is low (for example because of an old projector). Be prepared! For more information on color scales, have a look at this website. Scales and Axes Always label your axes clearly and use units when relevant. If one variable spans many orders of magnitude, consider using a log scale. Also check that your axis limits don’t exaggerate or minimize effects (avoid “truncating” axes unless you have a good reason). Faceting When comparing groups or categories, faceting (facet_wrap or facet_grid) is often clearer than putting everything into one crowded plot. It makes patterns easier to see across groups. Annotations Don’t be afraid to annotate! Adding text labels, arrows, or lines (like geom_text, geom_label, geom_vline) can help guide the reader to the most important takeaways. Themes Use themes (theme_minimal(), theme_classic(), etc.) to quickly adjust the overall look. If you’re preparing figures for teaching or publication, setting a consistent theme across all your figures improves readability. The cool thing is that some very interesting journals and platforms have their own ggplot-themes to be used in R. For this, have a look at the package ggthemes which includes theme_economist() and theme_wj() (Wall Street Journal), among others. Clarity in Legends Place legends where they don’t overlap with the data, and make sure the labels are descriptive. Sometimes it’s even better to replace the legend entirely with direct labeling inside the plot. 6.15 Conclusion ggplot2’s grammar of graphics provides a powerful and consistent framework for creating visualizations. The key principles to remember are: Build plots layer by layer - start with data and aesthetics, then add geoms, scales, and themes Map variables to aesthetics - use aes() to connect your data to visual properties Choose appropriate geoms - different geoms for different types of data and relationships Use faceting - create small multiples to show patterns across groups Customize with scales and themes - fine-tune appearance and mapping of aesthetic properties By mastering these concepts and functions, you can create publication-ready visualizations that effectively communicate insights from your data. 6.16 My favorite figures I ever made While in my internship, I was able to work with a huge dataset on detected enteroviruses in clinical settings all over Europe. I collected, harmonized, analyzed and visualized all that data which even got published! The first figure displays the number of detections per month throughout the study periods (with summer months in red) (fig A) and the relative intensity of specific virus types. A light color means that in that specific month, that specific enterovirus type was very common (fig B). The second figure shows a classic epicurve per specific virus type (in red) with the total number of detected enteroviruses per month in grey (Schrijver et al., 2025). Caption text Caption text "],["statistics.html", "Chapter 7 Statistics 7.1 Descriptive statistics and outbreak investigation 7.2 Inferential statistics in epidemiology 7.3 Inferential Statistics in Epidemiology", " Chapter 7 Statistics Broadly speaking, there’s two fields of statistics, namely descriptive statistics and inferential statistics. Descriptive statistics summarize and organize data to make it understandable, using measures like means, medians, proportions, or visualizations such as histograms and bar charts. In contrast, inferential statistics go a step further by using data from a sample to make conclusions or predictions about a larger population. This involves estimating parameters, testing hypotheses, and calculating measures. 7.1 Descriptive statistics and outbreak investigation 7.1.1 Introduction Outbreak investigations are critical components of public health practice. When people become sick from a common source, epidemiologists work quickly to identify the cause and prevent further illness. In this chapter, we’ll analyze data from a Salmonella Typhimurium outbreak to demonstrate how statistical analysis can help identify the source of foodborne illness. We’ll be working with the S.typh dataset from the Epi package, which contains data from a real outbreak investigation conducted as a matched case-control study. 7.1.2 Loading and Exploring the Data library(tidyverse) ## ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ── ## ✔ dplyr 1.1.4 ✔ readr 2.1.5 ## ✔ forcats 1.0.0 ✔ stringr 1.5.1 ## ✔ ggplot2 3.5.1 ✔ tibble 3.2.1 ## ✔ lubridate 1.9.3 ✔ tidyr 1.3.1 ## ✔ purrr 1.0.2 ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ── ## ✖ dplyr::filter() masks stats::filter() ## ✖ dplyr::lag() masks stats::lag() ## ℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors library(Epi) # Load the dataset data(S.typh) typhoid_data &lt;- S.typh # Inspect the structure of our data head(typhoid_data) ## id set case age sex abroad beef pork veal poultry liverp veg fruit egg plant7 ## 1 1 1 1 52 1 0 1 1 1 1 1 0 1 1 1 ## 2 2 1 0 52 1 0 1 0 0 0 1 1 1 0 0 ## 3 3 1 0 52 1 0 1 1 0 1 1 1 1 1 0 ## 4 4 2 1 41 1 0 1 NA NA NA 1 1 NA NA NA ## 5 5 2 0 41 1 0 1 1 0 0 1 1 0 1 1 ## 6 6 2 0 41 1 0 1 1 0 0 1 1 1 1 1 Tip: You can learn more about any dataset in R by typing ?S.typh in your console. This will show you detailed documentation about the variables and study design. 7.1.2.1 Understanding the Variables Our dataset contains the following variables: id: Person identification number set: Matched set indicator (used in the study design) case: Case-control status (1 = case/sick person, 0 = control/healthy person) age: Age of individual sex: Sex of individual (1 = male, 2 = female) abroad: Recent travel abroad in the last two weeks (1 = yes, 0 = no) Additionally, we have 9 food exposure variables indicating whether each person consumed specific foods: beef, pork, veal, poultry, liverp (liver pâté), veg (vegetables), fruit, egg, plant7. 7.1.3 Data Preparation: Creating a Tidy Dataset Currently, our food exposure data is in “wide” format with separate columns for each food type. For analysis and visualization, we need to convert this to “long” format where each row represents one person-food combination. As you learned in Chapter 5, we can use the pivot_longer() function for this transformation. # Define our food categories food_categories &lt;- c(&#39;beef&#39;, &#39;pork&#39;, &#39;veal&#39;, &#39;poultry&#39;, &#39;liverp&#39;, &#39;veg&#39;, &#39;fruit&#39;, &#39;egg&#39;, &#39;plant7&#39;) # Convert to long format typhoid_data_long &lt;- typhoid_data %&gt;% pivot_longer(cols = all_of(food_categories), names_to = &#39;food&#39;, values_to = &#39;eaten&#39;) %&gt;% mutate(eaten = eaten, food = as.factor(food), case = as.factor(case)) 7.1.4 Initial Visual Exploration Now that we have a long dataframe, we can create visualizations to explore patterns in food consumption among cases. Let’s start by examining the total number of cases that reported eating each food category. # Calculate total consumption per food category among all participants typhoid_cases_per_food &lt;- typhoid_data_long %&gt;% group_by(food) %&gt;% summarise(eaten = sum(eaten, na.rm = TRUE)) # Create bar plot ggplot(typhoid_cases_per_food) + geom_bar(aes(x = food, y = eaten), stat = &quot;identity&quot;, fill = &#39;brown3&#39;, color = &#39;black&#39;) + theme_minimal() + labs(x = &quot;Food Category&quot;, y = &quot;Number who ate this food&quot;, title = &quot;Total food consumption reported in the study&quot;, subtitle = &quot;All participants (cases and controls combined)&quot;) From this initial visualization, we can see that beef, egg, fruit, liver pâté (liverp), and plant7 were consumed by many participants, while poultry and veal were less commonly consumed. 7.1.5 Comparing Cases and Controls However, the key question in outbreak investigation is not just which foods were commonly eaten, but which foods show a difference between cases (sick people) and controls (healthy people). Let’s create a comparison visualization. # Calculate consumption by case status and food typhoid_category_per_food &lt;- typhoid_data_long %&gt;% group_by(case, food) %&gt;% summarise(eaten = sum(eaten, na.rm = TRUE), .groups = &#39;drop&#39;) # Create side-by-side bar plot ggplot(typhoid_category_per_food) + geom_bar(aes(x = food, y = eaten, fill = factor(case)), stat = &quot;identity&quot;, color = &#39;black&#39;, position = position_dodge()) + scale_fill_manual(values = c(&quot;0&quot; = &quot;grey&quot;, &quot;1&quot; = &quot;red&quot;), labels = c(&quot;0&quot; = &quot;Controls&quot;, &quot;1&quot; = &quot;Cases&quot;)) + theme_minimal() + theme(axis.text.x = element_text(angle = 45, hjust = 1)) + labs(x = &quot;Food Category&quot;, y = &quot;Number who ate this food&quot;, fill = &quot;Group&quot;, title = &quot;Food consumption by case status&quot;, subtitle = &quot;Comparing cases (sick) and controls (healthy)&quot;) Question for reflection: Looking at this figure, what does your initial assessment suggest about the potential source of the outbreak? Which foods show the largest differences between cases and controls? 7.1.6 Statistical Analysis: Attack Rates and Odds Ratios Visual exploration gives us initial insights, but we need statistical measures to quantify the strength of association between food exposures and illness. In outbreak investigations, epidemiologists use several key concepts: 7.1.6.1 Attack Rate The attack rate (AR) measures the proportion of people who become ill among those exposed to a particular food. It’s calculated as: \\[\\text{Attack Rate} = \\frac{\\text{Number of cases who ate the food}}{\\text{Total number who ate the food}} \\times 100\\%\\] For example, if 30 out of 100 people who ate a particular dish become ill, the attack rate for that food is 30%. 7.1.6.2 Odds Ratio While attack rates give us the probability of illness within exposed groups, the odds ratio (OR) quantifies how much more likely someone is to become ill if exposed to a particular food compared to someone who was not exposed. The OR is defined as: \\[\\text{OR} = \\frac{\\text{Odds of illness when exposed}}{\\text{Odds of illness when not exposed}}\\] Interpreting odds ratios: - OR ≈ 1: No association between exposure and illness - OR &lt; 1: Exposure associated with lower odds of illness (protective effect) - OR &gt; 1: Exposure associated with higher odds of illness (risk factor) For example, an OR of 2.5 means that people exposed to the food have 2.5 times the odds of becoming sick compared to those not exposed. 7.1.6.3 Weighted Odds Ratio To account for the practical impact of an exposure, we can compute a weighted odds ratio that combines the magnitude of association (OR) with the absolute number of sick individuals who were exposed. This helps identify foods that are both strongly associated with illness and affect a significant portion of cases. \\[\\text{Weighted OR} = \\text{OR} \\times \\text{Number of cases exposed}\\] 7.1.7 Calculating Statistics for Our Data Let’s calculate these measures for each food category in our outbreak: # Convert back to wide format for easier calculation typhoid_df &lt;- typhoid_data # Gather food columns into long format and calculate 2x2 tables typhoid_long &lt;- typhoid_df %&gt;% pivot_longer(cols = beef:plant7, names_to = &quot;food&quot;, values_to = &quot;ate&quot;) %&gt;% filter(!is.na(ate)) # Calculate odds ratios for each food food_analysis &lt;- typhoid_long %&gt;% group_by(food) %&gt;% summarise( # 2x2 table cells: A=sick&amp;ate, B=healthy&amp;ate, C=sick&amp;didn&#39;t eat, D=healthy&amp;didn&#39;t eat A = sum(ate == 1 &amp; case == 1), # cases who ate B = sum(ate == 1 &amp; case == 0), # controls who ate C = sum(ate == 0 &amp; case == 1), # cases who didn&#39;t eat D = sum(ate == 0 &amp; case == 0), # controls who didn&#39;t eat .groups = &#39;drop&#39; ) %&gt;% mutate( total_ate = A + B, attack_rate = round((A / total_ate) * 100, 1), OR = round((A * D) / (B * C), 2), weighted_OR = round(OR * A, 1) ) %&gt;% arrange(desc(weighted_OR)) %&gt;% select(food, A, total_ate, attack_rate, OR, weighted_OR) # Display results print(food_analysis) ## # A tibble: 9 × 6 ## food A total_ate attack_rate OR weighted_OR ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 plant7 29 62 46.8 4.48 130. ## 2 pork 33 98 33.7 1.46 48.2 ## 3 egg 31 98 31.6 1.03 31.9 ## 4 veal 5 7 71.4 5.88 29.4 ## 5 beef 28 88 31.8 0.84 23.5 ## 6 veg 40 122 32.8 0.49 19.6 ## 7 poultry 14 41 34.1 1.24 17.4 ## 8 fruit 32 113 28.3 0.23 7.4 ## 9 liverp 43 130 33.1 0.16 6.9 7.1.8 Interpretation of Results Looking at our statistical analysis: Plant7 shows by far the highest weighted odds ratio, indicating it is both strongly associated with illness (high OR) and affected many cases (high A). Attack rates help us understand the risk level - foods with high attack rates pose greater risk to those who consume them. Simple odds ratios show the strength of association, but may be misleading if very few people were exposed (like veal). Weighted odds ratios provide the most practical measure for outbreak investigation, balancing both statistical association and public health impact. 7.1.9 Conclusion Through this systematic analysis combining data visualization and statistical measures, we identified plant7 as the most likely source of the Salmonella Typhimurium outbreak. This finding was indeed confirmed in the actual investigation - demonstrating how epidemiological methods can effectively pinpoint outbreak sources and guide public health interventions. Key learning points: - Visual exploration provides initial insights but must be followed by statistical analysis - Multiple measures (attack rates, odds ratios, weighted odds ratios) provide different perspectives on food-illness associations - Weighted odds ratios are particularly useful in outbreak investigations as they consider both strength of association and public health impact - Systematic epidemiological analysis is essential for evidence-based outbreak response 7.2 Inferential statistics in epidemiology 7.3 Inferential Statistics in Epidemiology While descriptive statistics and visualizations help us explore patterns in our data, inferential statistics allow us to make conclusions about populations based on our sample data and assess whether observed differences are likely due to chance or represent real associations. In epidemiological investigations, we often want to know: “Are the differences we observe between cases and controls statistically significant?” 7.3.1 Understanding Statistical Significance Before diving into specific tests, let’s establish key concepts: Null hypothesis (H₀): Usually states there is no difference or association Alternative hypothesis (H₁): States there is a difference or association P-value: The probability of observing our results (or more extreme) if the null hypothesis were true Significance level (α): The threshold below which we reject the null hypothesis (commonly 0.05) Confidence interval: A range of values that likely contains the true population parameter 7.3.2 Comparing Demographics Between Cases and Controls Let’s start by examining whether there are significant demographic differences between our typhoid cases and controls. This is important because demographic differences might confound our food exposure analysis. 7.3.2.1 Testing Age Differences with T-Tests A t-test compares means between two groups. We’ll use it to test whether cases and controls differ significantly in age. # First, let&#39;s examine age distributions typhoid_data %&gt;% group_by(case) %&gt;% summarise( n = n(), mean_age = round(mean(age, na.rm = TRUE), 1), sd_age = round(sd(age, na.rm = TRUE), 1), median_age = median(age, na.rm = TRUE), .groups = &#39;drop&#39; ) ## # A tibble: 2 × 5 ## case n mean_age sd_age median_age ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 89 25.5 17.6 18 ## 2 1 47 24.1 17.7 17 Now let’s visualize the age distributions: # Create boxplots to compare age distributions ggplot(typhoid_data, aes(x = factor(case), y = age, fill = factor(case))) + geom_boxplot(alpha = 0.7) + geom_jitter(width = 0.2, alpha = 0.5) + scale_fill_manual(values = c(&quot;0&quot; = &quot;grey&quot;, &quot;1&quot; = &quot;red&quot;), labels = c(&quot;0&quot; = &quot;Controls&quot;, &quot;1&quot; = &quot;Cases&quot;)) + scale_x_discrete(labels = c(&quot;0&quot; = &quot;Controls&quot;, &quot;1&quot; = &quot;Cases&quot;)) + theme_minimal() + labs(x = &quot;Group&quot;, y = &quot;Age (years)&quot;, fill = &quot;Group&quot;, title = &quot;Age distribution by case status&quot;, subtitle = &quot;Comparing age between cases and controls&quot;) ## Warning: Removed 19 rows containing non-finite outside the scale range ## (`stat_boxplot()`). ## Warning: Removed 19 rows containing missing values or values outside the scale range ## (`geom_point()`). Now let’s perform the statistical test: # Perform independent samples t-test age_test &lt;- t.test(age ~ case, data = typhoid_data) print(age_test) ## ## Welch Two Sample t-test ## ## data: age by case ## t = 0.3888, df = 84.615, p-value = 0.6984 ## alternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0 ## 95 percent confidence interval: ## -5.446378 8.093997 ## sample estimates: ## mean in group 0 mean in group 1 ## 25.46667 24.14286 # Extract key information age_diff &lt;- round(age_test$estimate[2] - age_test$estimate[1], 2) p_value &lt;- round(age_test$p.value, 4) ci_lower &lt;- round(age_test$conf.int[1], 2) ci_upper &lt;- round(age_test$conf.int[2], 2) cat(&quot;Mean age difference (cases - controls):&quot;, age_diff, &quot;years\\n&quot;) ## Mean age difference (cases - controls): -1.32 years cat(&quot;95% Confidence interval: [&quot;, ci_lower, &quot;,&quot;, ci_upper, &quot;]\\n&quot;) ## 95% Confidence interval: [ -5.45 , 8.09 ] cat(&quot;P-value:&quot;, p_value, &quot;\\n&quot;) ## P-value: 0.6984 Interpretation: If the p-value is &lt; 0.05, we conclude there’s a statistically significant difference in age between cases and controls. The confidence interval tells us the range of plausible values for the true age difference in the population. 7.3.2.2 Testing Sex Differences with Chi-Square Tests For categorical variables like sex, we use chi-square tests to test for associations. # Create a contingency table sex_table &lt;- table(typhoid_data$case, typhoid_data$sex) print(sex_table) ## ## 1 2 ## 0 49 40 ## 1 25 22 # Add row and column labels for clarity dimnames(sex_table) &lt;- list( Case_Status = c(&quot;Controls&quot;, &quot;Cases&quot;), Sex = c(&quot;Male&quot;, &quot;Female&quot;) ) print(sex_table) ## Sex ## Case_Status Male Female ## Controls 49 40 ## Cases 25 22 # Calculate proportions prop_table &lt;- prop.table(sex_table, margin = 1) # Proportions by row round(prop_table, 3) ## Sex ## Case_Status Male Female ## Controls 0.551 0.449 ## Cases 0.532 0.468 Now perform the chi-square test: # Chi-square test for independence sex_test &lt;- chisq.test(sex_table) print(sex_test) ## ## Pearson&#39;s Chi-squared test with Yates&#39; continuity correction ## ## data: sex_table ## X-squared = 0.00070864, df = 1, p-value = 0.9788 # Extract key information chi_sq &lt;- round(sex_test$statistic, 3) p_value_sex &lt;- round(sex_test$p.value, 4) df &lt;- sex_test$parameter cat(&quot;Chi-square statistic:&quot;, chi_sq, &quot;\\n&quot;) ## Chi-square statistic: 0.001 cat(&quot;Degrees of freedom:&quot;, df, &quot;\\n&quot;) ## Degrees of freedom: 1 cat(&quot;P-value:&quot;, p_value_sex, &quot;\\n&quot;) ## P-value: 0.9788 Interpretation: The chi-square test tells us whether the distribution of sex differs significantly between cases and controls. A significant result (p &lt; 0.05) would suggest that sex is associated with case status. 7.3.3 Testing Food Exposures: Statistical Significance of Associations Now let’s add statistical testing to our food exposure analysis. We’ll test whether each food exposure is significantly associated with illness. # Function to perform chi-square test for each food test_food_association &lt;- function(food_name) { # Create 2x2 table for this food food_col &lt;- typhoid_data[[food_name]] case_col &lt;- typhoid_data$case # Remove missing values complete_cases &lt;- !is.na(food_col) &amp; !is.na(case_col) food_clean &lt;- food_col[complete_cases] case_clean &lt;- case_col[complete_cases] # Create contingency table food_table &lt;- table(case_clean, food_clean) # Perform chi-square test if(all(food_table &gt;= 5)) { # Check if chi-square assumptions are met test_result &lt;- chisq.test(food_table) return(list( food = food_name, chi_sq = round(test_result$statistic, 3), p_value = round(test_result$p.value, 4), test_type = &quot;chi-square&quot; )) } else { # Use Fisher&#39;s exact test for small cell counts test_result &lt;- fisher.test(food_table) return(list( food = food_name, p_value = round(test_result$p.value, 4), test_type = &quot;Fisher&#39;s exact&quot; )) } } # Apply to all food variables food_categories &lt;- c(&#39;beef&#39;, &#39;pork&#39;, &#39;veal&#39;, &#39;poultry&#39;, &#39;liverp&#39;, &#39;veg&#39;, &#39;fruit&#39;, &#39;egg&#39;, &#39;plant7&#39;) food_tests &lt;- map_dfr(food_categories, test_food_association) ## Warning in chisq.test(food_table): Chi-squared approximation may be incorrect print(food_tests) ## # A tibble: 9 × 4 ## food chi_sq p_value test_type ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 beef 0.059 0.809 chi-square ## 2 pork 0.358 0.549 chi-square ## 3 veal NA 0.0343 Fisher&#39;s exact ## 4 poultry 0.103 0.748 chi-square ## 5 liverp NA 0.117 Fisher&#39;s exact ## 6 veg 0.573 0.449 chi-square ## 7 fruit 7.38 0.0066 chi-square ## 8 egg 0 1 chi-square ## 9 plant7 11.7 0.0006 chi-square 7.3.4 Confidence Intervals for Odds Ratios Let’s enhance our previous odds ratio analysis by adding confidence intervals and p-values: # Enhanced function to calculate OR with confidence intervals calculate_or_with_ci &lt;- function(food_name) { # Create 2x2 table food_col &lt;- typhoid_data[[food_name]] case_col &lt;- typhoid_data$case # Remove missing values complete_cases &lt;- !is.na(food_col) &amp; !is.na(case_col) food_clean &lt;- food_col[complete_cases] case_clean &lt;- case_col[complete_cases] # Create table: rows = case status, columns = food exposure tab &lt;- table(case_clean, food_clean) # Extract 2x2 table values a &lt;- tab[2,2] # cases exposed b &lt;- tab[1,2] # controls exposed c &lt;- tab[2,1] # cases not exposed d &lt;- tab[1,1] # controls not exposed # Calculate OR or &lt;- (a * d) / (b * c) # Calculate 95% CI for OR using log transformation log_or &lt;- log(or) se_log_or &lt;- sqrt(1/a + 1/b + 1/c + 1/d) ci_lower &lt;- exp(log_or - 1.96 * se_log_or) ci_upper &lt;- exp(log_or + 1.96 * se_log_or) # Chi-square test chi_test &lt;- chisq.test(tab) return(data.frame( food = food_name, cases_exposed = a, cases_total = a + c, OR = round(or, 2), CI_lower = round(ci_lower, 2), CI_upper = round(ci_upper, 2), p_value = round(chi_test$p.value, 4), significant = chi_test$p.value &lt; 0.05 )) } # Calculate for all foods or_results &lt;- map_dfr(food_categories, calculate_or_with_ci) ## Warning in chisq.test(tab): Chi-squared approximation may be incorrect ## Warning in chisq.test(tab): Chi-squared approximation may be incorrect ## Warning in chisq.test(tab): Chi-squared approximation may be incorrect # Sort by OR and display or_results_sorted &lt;- or_results %&gt;% arrange(desc(OR)) %&gt;% mutate( CI_text = paste0(&quot;[&quot;, CI_lower, &quot;, &quot;, CI_upper, &quot;]&quot;), significance = ifelse(significant, &quot;Yes&quot;, &quot;No&quot;) ) %&gt;% select(food, cases_exposed, cases_total, OR, CI_text, p_value, significance) print(or_results_sorted) ## food cases_exposed cases_total OR CI_text p_value significance ## 1 veal 5 42 5.88 [1.09, 31.68] 0.0604 No ## 2 plant7 29 39 4.48 [1.93, 10.4] 0.0006 Yes ## 3 pork 33 41 1.46 [0.59, 3.62] 0.5494 No ## 4 poultry 14 40 1.24 [0.56, 2.73] 0.7477 No ## 5 egg 31 40 1.03 [0.42, 2.52] 1.0000 No ## 6 beef 28 43 0.84 [0.39, 1.82] 0.8086 No ## 7 veg 40 45 0.49 [0.13, 1.78] 0.4490 No ## 8 fruit 32 44 0.23 [0.08, 0.64] 0.0066 Yes ## 9 liverp 43 46 0.16 [0.02, 1.63] 0.2283 No 7.3.5 Interpreting the Results Let’s create a visualization of our odds ratios with confidence intervals: # Create forest plot of odds ratios or_results %&gt;% mutate(food = reorder(food, OR)) %&gt;% ggplot(aes(x = food, y = OR)) + geom_point(aes(color = significant), size = 3) + geom_errorbar(aes(ymin = CI_lower, ymax = CI_upper, color = significant), width = 0.2, size = 1) + geom_hline(yintercept = 1, linetype = &quot;dashed&quot;, color = &quot;red&quot;, alpha = 0.7) + scale_color_manual(values = c(&quot;TRUE&quot; = &quot;red&quot;, &quot;FALSE&quot; = &quot;grey&quot;), labels = c(&quot;TRUE&quot; = &quot;Significant&quot;, &quot;FALSE&quot; = &quot;Not significant&quot;)) + scale_y_log10() + # Log scale for better visualization coord_flip() + theme_minimal() + labs( x = &quot;Food Category&quot;, y = &quot;Odds Ratio (log scale)&quot;, color = &quot;Statistically\\nSignificant&quot;, title = &quot;Odds Ratios for Food Exposures with 95% Confidence Intervals&quot;, subtitle = &quot;Dashed line represents OR = 1 (no association)&quot; ) + theme(legend.position = &quot;bottom&quot;) ## Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0. ## ℹ Please use `linewidth` instead. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was ## generated. 7.3.6 Key Statistical Concepts for Interpretation Odds Ratio Confidence Intervals: - If the 95% CI includes 1.0, the association is not statistically significant - If the entire CI is above 1.0, the food is a significant risk factor - If the entire CI is below 1.0, the food is significantly protective P-values in Context: - P &lt; 0.05: Evidence against the null hypothesis of no association - P ≥ 0.05: Insufficient evidence to reject the null hypothesis - Important: “Not significant” doesn’t mean “no effect” - it means insufficient evidence Multiple Testing Considerations: When testing multiple food exposures, we increase the chance of finding significant results by chance alone. In practice, epidemiologists might adjust for multiple comparisons or focus on the strongest associations with biological plausibility. 7.3.7 Summary Through inferential statistics, we’ve: Tested demographic differences: Determined whether cases and controls are comparable in age and sex distribution Quantified food associations: Calculated odds ratios with confidence intervals for each food exposure Assessed statistical significance: Used p-values to evaluate the evidence for each association Visualized uncertainty: Created forest plots showing both point estimates and confidence intervals This statistical framework provides the foundation for making evidence-based conclusions in epidemiological investigations. The combination of effect size measures (like odds ratios), confidence intervals, and statistical significance tests gives us a comprehensive picture of associations in our data. Key takeaway: Statistical significance testing helps us distinguish between associations that are likely real versus those that might be due to random variation, but it should always be interpreted alongside biological plausibility and public health importance. "],["working-with-epidemiological-data.html", "Chapter 8 Working with epidemiological data 8.1 Acessing data 8.2 (Imperfect) epidemiological data 8.3 Case Study 1: Yellow Fever", " Chapter 8 Working with epidemiological data Data is at the heart of epidemiology. We look at where and when diseases occur, to hopefully better understand their origin or their cause, how to prevent or cure the disease, or to make predictions about the future. That’s why epidemiologists need to be able to work with disease data, and know how and where to look for it! That’s why in this chapter we go over some data-extraction, processing, visualization and analysis. We start simple, with an example of Yellow Fever, and then get into more complex case in which we try to understand cholera outbreaks. 8.1 Acessing data Coming soon … raw data cleaned data harmonization -&gt; standardization data cleaning: fixing or removing incorrect, inconsistent or missing data data harmonization: standardization or alignment of data 8.2 (Imperfect) epidemiological data Coming soon … 8.3 Case Study 1: Yellow Fever Yellow fever (YF) is a severe and sometimes fatal hemorrhagic fever caused by a flavivirus with a single-stranded, negatively oriented RNA genome. It primarily affects regions in Africa and Latin America, where the virus sustains itself through a sylvatic cycle involving wild mosquitoes and non-human primates. In Brazil, for instance, mosquito species like Haemagogus and Sabethes maintain this cycle in forested areas. While a highly effective vaccine exists, yellow fever outbreaks still occur, particularly when the virus spills over into urban settings (Chippaux &amp; Chippaux, 2018). Historically, yellow fever was confined to Africa until the transatlantic slave trade introduced it to Latin America, where favorable climates and mosquito vectors allowed it to take root. Cities like New Orleans suffered repeated, devastating outbreaks in the past. Brazil saw a decline in cases mid-20th century, but a resurgence occurred between 2016 and 2018, with over 2,000 reported cases and a strikingly high death toll of 681. This resurgence was partly driven by the spread of the virus into urban areas, facilitated by Aedes aegypti and Aedes albopictus, mosquitoes that thrive in cities. Curiously, yellow fever has never established itself in Asia despite the presence of competent vectors and similar arboviruses, a mystery that may relate to immunological cross-reactivity with other flaviviruses like dengue (Chippaux &amp; Chippaux, 2018). The WHO has data data available for download for many different things, including casenumbers for many infectious diseases, including that for YF. The raw data for YF you can find here. I have cleaned this data a little so that the dataframe is as concise as possible, and so that all the countrynames correctly map to the ones in the world_shapefile dataset. 8.3.0.1 Installing packages library(scales) library(ggplot2) library(readr) ## ## Attaching package: &#39;readr&#39; ## The following object is masked from &#39;package:scales&#39;: ## ## col_factor library(glue) # The following packages print a lot of noise when installing so I silence their output # by wrapping them in a `suppressPackageStartupMessages({}) suppressPackageStartupMessages({ library(tidyverse) library(gapminder) library(rnaturalearth) library(rnaturalearthdata) library(sf) }) 8.3.0.2 Installing world shapedata Eventually, I want to show you how to make some plots and some maps. For that, we need to load the shape of the world, so that we can fill in countries with certain colors and such. For that we’ll load the world shapefile from the package rnaturalearth. This dataset includes many different columns, but we only require some of them; name, geometry and continent. world_shapefile = ne_countries(scale = &quot;medium&quot;, returnclass = &quot;sf&quot;) %&gt;% select(c(&quot;name&quot;,&quot;geometry&quot;, &quot;continent&quot;)) colnames(world_shapefile)[1] = c(&quot;country&quot;) continents_list = unique(world_shapefile$continent) countries_continents &lt;- world_shapefile %&gt;% st_set_geometry(NULL) %&gt;% select(country, continent) # World map ggplot(world_shapefile) + geom_sf(aes(fill = continent), color = &quot;black&quot;, linewidth = 0.01) + theme_minimal(base_size = 14) + labs( title = &#39;world shapefile&#39;, ) + theme( panel.background = element_rect(fill = &quot;#f7f9fc&quot;), legend.position = &quot;right&quot;, plot.title = element_text(face = &quot;bold&quot;, size = 16), plot.subtitle = element_text(size = 12) ) 8.3.0.3 Yellow Fever data who_yellowfever = read_csv(&quot;data/cleaned_yellowfever.csv&quot;, show_col_types = FALSE) # head allows us to preview the table head(who_yellowfever) ## # A tibble: 6 × 3 ## country year cases ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Algeria 2024 0 ## 2 Andorra 2024 0 ## 3 Anguilla 2024 0 ## 4 Antigua and Barb. 2024 0 ## 5 Armenia 2024 0 ## 6 Aruba 2024 0 # draw up some specifications of the dataset n_countries = length(unique(who_yellowfever$country)) first_year = min(who_yellowfever$year) last_year = max(who_yellowfever$year) glue(&quot;\\nThe dataframe contains yearly casedata for {n_countries} countries between {first_year} and {last_year}&quot;) ## The dataframe contains yearly casedata for 212 countries between 1974 and 2024 8.3.0.4 Global yearly cases Let’s first get a global overview of YF in the world the past 50 years. To start with, we will create a graph showing the number of cases in the world per year, and then we’ll show it per continent to see which are the ones most affected. # Global epicurve global_yearly_casenumbers &lt;- who_yellowfever %&gt;% group_by(year) %&gt;% summarise(cases = sum(cases)) ggplot(global_yearly_casenumbers) + geom_line(aes(x = year, y = cases), color = &#39;black&#39;, linewidth = 1.5) + # black outline geom_line(aes(x = year, y = cases), color = &#39;#ffcc00&#39;, linewidth = 1) + # yellow line scale_x_continuous(breaks = pretty_breaks(n = 20)) + # I don&#39;t want a break per year. I do a max of 20 year theme_classic(base_size = 14) + labs( x = &quot;&quot;, title = &quot;Yellow Fever Cases in the world 1974 - 2024&quot;, subtitle = &quot;number of cases reported to the WHO&quot; ) 8.3.0.5 Yearly cases per continent yearly_casenumbers_continent = merge(who_yellowfever, countries_continents, by = &quot;country&quot;) %&gt;% group_by(year, continent) %&gt;% summarise(cases = sum(cases), .groups = &quot;drop&quot;) %&gt;% filter(continent %in% continents_list) ggplot(yearly_casenumbers_continent) + geom_line(aes(x = year, y = cases, color = continent), linewidth = 1.5) + scale_x_continuous(breaks = pretty_breaks(n = 20)) + theme_classic(base_size = 14) + scale_color_brewer(palette = &quot;Dark2&quot;) + # 7 distinct colors labs( x = &quot;&quot;, title = &quot;Yellow Fever Cases in the world 1973 - 2024 per continent&quot;, subtitle = &quot;number of cases reported to the WHO&quot; ) + theme(legend.position = &quot;bottom&quot;) We can clearly tell that since 1974, Africa has experienced the largest burden by YF, though South America suffered between 2016 and 2019 as well. Let’s have a look at the total sum of cases per country in the past, to see which countries and which areas are affected the most. 8.3.0.6 Historical burden cases_per_country = who_yellowfever%&gt;% group_by(country) %&gt;% summarise(cases = sum(cases)) mapdata = st_as_sf(merge(cases_per_country, world_shapefile, on = &#39;country&#39;, all.y = TRUE)) mapdata$cases_category &lt;- cut( mapdata$cases, breaks = c(0,100, 500, 1000, 2000, 5000, 10000, 20000, Inf), # 8 breaks → 7 intervals labels = c(&quot;0 - 100&quot;, &quot;101-500&quot;,&quot;501-1000&quot;, &quot;1001-2000&quot;, &quot;2001-5000&quot;, &quot;5001-10000&quot;, &quot;10001-20000&quot;, &quot;&gt;20000&quot;), include.lowest = TRUE ) pl &lt;- ggplot(mapdata) + geom_sf(aes(fill = cases_category), color = &quot;black&quot;, size = 0.1) + scale_fill_brewer(palette = &quot;YlOrRd&quot;, na.value = &quot;gray95&quot;, name = &quot;Cases&quot;) + theme_minimal(base_size = 14) + theme( panel.background = element_rect(fill = &quot;#E0FFFF&quot;), # clean light background legend.position = &quot;right&quot;, plot.title = element_text(face = &quot;bold&quot;, size = 16), plot.subtitle = element_text(size = 12) ) + labs(title = &#39;Cumulative cases 1974 - 2024&#39;) pl From this we can clearly see that the most affected regions are central and west Africa, as well as central(?) Latin America. This does not allow us to look into temporal dynamics though, so let’s look a bit further into that using a function. 8.3.0.7 Country-specific epicurves plot_country = function(country_filter){ filtered_df &lt;- who_yellowfever %&gt;% filter(country == country_filter) pl = ggplot(filtered_df) + geom_line(aes(x = year, y = cases), color = &#39;black&#39;, linewidth = 1.5) + # black outline geom_line(aes(x = year, y = cases), color = &#39;#ffcc00&#39;, linewidth = 1) + # yellow line geom_point(aes(x = year, y = cases), color = &#39;black&#39;, linewidth = 1) + # black outline scale_x_continuous(breaks = pretty_breaks(n = 20)) + theme_classic(base_size = 14) + labs(x = &quot;&quot;, y=&#39;cases&#39;) + labs( title = paste0(&quot;Yellow Fever Cases in &quot;, country_filter, &quot; 1973 - 2024&quot;), subtitle = &quot;number of cases reported to the WHO&quot; ) return(pl) } plot_country(&quot;Dem. Rep. Congo&quot;) ## Warning in geom_point(aes(x = year, y = cases), color = &quot;black&quot;, linewidth = ## 1): Ignoring unknown parameters: `linewidth` plot_country(&quot;Chad&quot;) ## Warning in geom_point(aes(x = year, y = cases), color = &quot;black&quot;, linewidth = ## 1): Ignoring unknown parameters: `linewidth` plot_country(&quot;Nigeria&quot;) ## Warning in geom_point(aes(x = year, y = cases), color = &quot;black&quot;, linewidth = ## 1): Ignoring unknown parameters: `linewidth` plot_country(&quot;Ghana&quot;) ## Warning in geom_point(aes(x = year, y = cases), color = &quot;black&quot;, linewidth = ## 1): Ignoring unknown parameters: `linewidth` plot_country(&quot;Brazil&quot;) ## Warning in geom_point(aes(x = year, y = cases), color = &quot;black&quot;, linewidth = ## 1): Ignoring unknown parameters: `linewidth` plot_country(&quot;Peru&quot;) ## Warning in geom_point(aes(x = year, y = cases), color = &quot;black&quot;, linewidth = ## 1): Ignoring unknown parameters: `linewidth` This is actually already interesting! We can see that some many of these countries have very big outbreaks in a specific year, or over a couple of years, while a peak in one country doesn’t necessarily translate in one elsewhere. Peru, for example, had a big outbreak in 1995, and hasn’t had many cases since. Brazil, however, did not report manby cases until 2017 when it detected a huge outbreak that lasted until 2018. "],["case-study-dengue-fever-in-brazil.html", "Chapter 9 Case Study: Dengue Fever in Brazil 9.1 Introduction 9.2 Data Sources and Structure 9.3 Data Import and Preparation 9.4 Temporal Analysis: National Dengue Trends 9.5 Spatial Analysis: Environmental Context 9.6 Temporal-Spatial Analysis: Annual Incidence Maps 9.7 Climate Oscillations: Large-Scale Environmental Drivers 9.8 Ecological Analysis: Incidence by Environmental Zones 9.9 Seasonal Analysis: Weekly Patterns by Biome 9.10 Discussion and Key Findings 9.11 Further Analysis Opportunities", " Chapter 9 Case Study: Dengue Fever in Brazil 9.1 Introduction Dengue is a mosquito-borne viral disease that affects millions of people worldwide every year. It is primarily transmitted by the Aedes aegypti mosquito, which thrives in tropical and subtropical climates. Globally, dengue is a growing public health concern: the World Health Organization estimates that nearly half of the world’s population is at risk, and this number keeps increasing. Outbreaks are known to occur in over 100 countries. The disease ranges from mild flu-like symptoms to severe forms, including dengue hemorrhagic fever, which can be life-threatening. Brazil represents a particularly important context for studying dengue. Its vast territory spans multiple climatic zones, from tropical rainforests to semi-arid regions, and contains a diverse range of biomes. Seasonal rainfall patterns, temperature variations, and population density all contribute to the dynamics of dengue transmission across municipalities. Additionally, large-scale climate phenomena such as El Niño and La Niña can influence local weather conditions, indirectly affecting mosquito populations and disease incidence. In this case study, you will explore a rich dataset containing weekly dengue case counts, climate variables, population data, and ecological information for municipalities across Brazil. Using this data, we will investigate patterns of disease incidence, examine the relationship between environmental factors and dengue transmission, and apply statistical and modeling tools to draw insights. Learning objectives: Analyze temporal and spatial patterns of dengue incidence across different ecological zones Explore the relationship between environmental factors and disease transmission Apply data visualization techniques to identify epidemiological patterns Integrate multiple data sources for comprehensive epidemiological analysis 9.2 Data Sources and Structure This analysis uses data from the Mosqlimate project (https://sprint.mosqlimate.org/data/), which provides comprehensive surveillance data for vector-borne diseases in Brazil. Our dataset includes: Case data: Weekly dengue case counts by municipality (2010-2025) Population data: Annual population estimates for incidence calculations Climate data: Temperature, precipitation, and humidity measurements Environmental data: Biome classifications and Köppen climate zones Ocean climate data: Large-scale climate oscillation indices (El Niño, La Niña) Geographic data: Shapefiles for mapping and spatial analysis 9.3 Data Import and Preparation library(tidyverse) library(sf) library(lubridate) # Set data path compressed_data_path &lt;- &quot;data/dengue_compressed/&quot; # Import datasets population_data &lt;- read.csv(paste0(compressed_data_path, &quot;datasus_population_2001_2024.csv.gz&quot;)) case_data &lt;- read.csv(paste0(compressed_data_path, &quot;dengue.csv.gz&quot;)) climate_data &lt;- read.csv(paste0(compressed_data_path, &quot;climate.csv.gz&quot;)) environment_data &lt;- read.csv(paste0(compressed_data_path, &quot;environ_vars.csv.gz&quot;)) ocean_data &lt;- read.csv(paste0(compressed_data_path, &quot;ocean_climate_oscillations.csv.gz&quot;)) # Remove unnecessary columns from case data cols_to_remove &lt;- c(&#39;train_1&#39;, &#39;target_1&#39;, &#39;train_2&#39;, &#39;target_2&#39;, &#39;train_3&#39;, &#39;target_3&#39;, &#39;epiweek&#39;) case_data &lt;- case_data %&gt;% select(-all_of(cols_to_remove)) 9.3.1 Data Type Standardization To ensure consistent analysis, we’ll standardize column types across all datasets: change_columntypes &lt;- function(dataframe) { # Convert date column to Date object if present if (&#39;date&#39; %in% colnames(dataframe)) { dataframe &lt;- dataframe %&gt;% mutate(date = as.Date(date)) } # Convert geographic and categorical variables to factors factor_columns &lt;- c(&#39;geocode&#39;, &#39;uf&#39;, &#39;macroregional_geocode&#39;, &#39;regional_geocode&#39;, &#39;uf_code&#39;, &#39;koppen&#39;, &#39;biome&#39;) for (column in factor_columns) { if (column %in% colnames(dataframe)) { dataframe &lt;- dataframe %&gt;% mutate(!!column := as.factor(.data[[column]])) } } return(dataframe) } # Apply standardization to all datasets population_data &lt;- change_columntypes(population_data) case_data &lt;- change_columntypes(case_data) climate_data &lt;- change_columntypes(climate_data) environment_data &lt;- change_columntypes(environment_data) ocean_data &lt;- change_columntypes(ocean_data) 9.3.2 Geographic Data Import # Import shapefiles for mapping shapefile_municipalities &lt;- st_read(paste0(compressed_data_path, &#39;shape_muni.gpkg&#39;)) shapefile_regions &lt;- st_read(paste0(compressed_data_path, &#39;shape_regional_health.gpkg&#39;)) shapefile_macroregions &lt;- st_read(paste0(compressed_data_path, &#39;shape_macroregional_health.gpkg&#39;)) # Ensure consistent data types for geographic identifiers shapefile_municipalities$geocode &lt;- as.factor(shapefile_municipalities$geocode) shapefile_municipalities$uf_code &lt;- as.factor(shapefile_municipalities$uf_code) 9.4 Temporal Analysis: National Dengue Trends Let’s begin by examining the overall temporal patterns of dengue incidence in Brazil: # Calculate national case totals by week national_cases &lt;- case_data %&gt;% group_by(date) %&gt;% summarise(cases = sum(casos), .groups = &#39;drop&#39;) # Visualize national trends ggplot(national_cases, aes(x = date, y = cases)) + geom_line(color = &quot;black&quot;, linewidth = 0.75) + scale_x_date(date_breaks = &quot;1 year&quot;, date_labels = &quot;%Y&quot;) + labs(x = &quot;Year&quot;, y = &quot;Weekly Cases&quot;, title = &quot;Dengue Cases in Brazil - National Trends&quot;, subtitle = &quot;Weekly reported cases 2010-2025&quot;) + theme_bw() + theme(axis.text.x = element_text(angle = 45, hjust = 1)) You’ll note that while Dengue has been a consistent burden over the past two decades in Brazil, interestingly with the exceptions of 2017 and 2018, there was an unprecedented peak of cases in 2024. What is fascinating is that no one really knows why that was… 9.5 Spatial Analysis: Environmental Context Brazil’s diverse geography creates distinct ecological zones that influence dengue transmission. Let’s visualize the environmental context: library(patchwork) library(RColorBrewer) # Join environmental data with geographic boundaries map_environment_data &lt;- shapefile_municipalities %&gt;% left_join(environment_data, by = c(&quot;geocode&quot;, &quot;uf_code&quot;)) # Plot 1: Brazilian states p1 &lt;- ggplot() + geom_sf(data = map_environment_data, aes(fill = uf_code), color = &#39;grey&#39;, linewidth = 0.05) + geom_sf(data = shapefile_regions, fill = NA, color = &quot;black&quot;, linewidth = 0.5, linetype = &#39;dashed&#39;) + geom_sf(data = shapefile_macroregions, fill = NA, color = &quot;black&quot;, linewidth = 1) + scale_fill_viridis_d(option = &quot;turbo&quot;) + theme_bw() + labs(title = &quot;Administrative Units - Brazilian States&quot;, fill = &#39;State Code&#39;) + theme(legend.position = &quot;none&quot;) # Too many states for readable legend # Plot 2: Biome classification p2 &lt;- ggplot() + geom_sf(data = map_environment_data, aes(fill = biome), color = &quot;grey&quot;, linewidth = 0.05) + geom_sf(data = shapefile_macroregions, fill = NA, color = &quot;black&quot;, linewidth = 1) + scale_fill_brewer(palette = &#39;Set3&#39;) + theme_bw() + labs(title = &quot;Biome Classification&quot;, fill = &quot;Biome&quot;) # Plot 3: Köppen climate zones p3 &lt;- ggplot() + geom_sf(data = map_environment_data, aes(fill = koppen), color = &quot;grey&quot;, linewidth = 0.05) + geom_sf(data = shapefile_macroregions, fill = NA, color = &quot;black&quot;, linewidth = 1) + scale_fill_brewer(palette = &#39;Paired&#39;) + theme_bw() + labs(title = &quot;Köppen Climate Classification&quot;, fill = &quot;Climate Zone&quot;) # Combine plots combined_plot &lt;- p1 / p2 / p3 combined_plot 9.6 Temporal-Spatial Analysis: Annual Incidence Maps To understand how dengue incidence varies across space and time, we’ll create annual incidence maps: # Calculate yearly incidence rates yearly_case_data &lt;- case_data %&gt;% mutate(year = year(date)) %&gt;% group_by(year, geocode) %&gt;% summarise(cases = sum(casos), .groups = &#39;drop&#39;) yearly_incidence &lt;- yearly_case_data %&gt;% left_join(population_data, by = c(&#39;year&#39;, &#39;geocode&#39;)) %&gt;% mutate(incidence = cases / population * 10000, geocode = as.factor(geocode)) # Join with geographic data for mapping map_incidence_data &lt;- shapefile_municipalities %&gt;% left_join(yearly_incidence, by = &quot;geocode&quot;) # Create annual incidence maps ggplot(map_incidence_data, aes(fill = incidence)) + geom_sf(color = NA) + theme_bw() + facet_wrap(~ year, ncol = 4) + scale_fill_viridis_c(option = &quot;magma&quot;, trans = &quot;sqrt&quot;, na.value = &quot;grey80&quot;) + theme( legend.title = element_text(size = 16), legend.text = element_text(size = 14), legend.key.width = unit(1, &quot;cm&quot;), legend.key.height = unit(8, &quot;cm&quot;), axis.text = element_blank(), axis.ticks = element_blank() ) + labs(title = &#39;Annual Dengue Incidence per Municipality in Brazil&#39;, subtitle = &#39;2010-2024 (incidence per 10,000 population, square root scale)&#39;, fill = &quot;Incidence\\n(per 10,000)&quot;) What you’ll see here is that throughout the years, different regions were affected. The situation in 2024 was one in which many different municipalities suffered extremely in comparison to previous years, though there’s definitely a cluster in the south east of Brazil. 9.7 Climate Oscillations: Large-Scale Environmental Drivers Large-scale climate phenomena like El Niño and La Niña can influence regional weather patterns and dengue transmission, which has been established in some literature. Each ocean has its own index (a single value) reflecting the state its climate is in. Lets look at how that varied over time. # Prepare ocean climate data for visualization ocean_data_long &lt;- ocean_data %&gt;% pivot_longer(cols = -date, names_to = &#39;ocean_variable&#39;, values_to = &#39;value&#39;) %&gt;% mutate(date = as.Date(date), ocean_variable = as.factor(ocean_variable)) # Plot climate oscillation indices ggplot(ocean_data_long, aes(x = date, y = value, color = ocean_variable)) + geom_line(linewidth = 1) + facet_wrap(~ocean_variable, nrow = 3, scales = &quot;free_y&quot;) + theme_bw() + labs(title = &quot;Ocean Climate Oscillation Indices&quot;, subtitle = &quot;Large-scale climate patterns affecting regional weather&quot;, x = &quot;Date&quot;, y = &quot;Index Value&quot;) + theme(legend.position = &quot;none&quot;) 9.8 Ecological Analysis: Incidence by Environmental Zones Now let’s examine how dengue incidence varies across different biomes and climate zones. Note that the big outbreak observed in 2024 was not seen for every biome in Brazil. # Prepare data with environmental classifications cases_environment_data &lt;- case_data %&gt;% mutate(year = year(date)) %&gt;% filter(year &lt; 2025) %&gt;% left_join(population_data, by = c(&#39;year&#39;, &#39;geocode&#39;)) %&gt;% left_join(environment_data, by = &#39;geocode&#39;) # Calculate incidence rates by biome incidence_rates_by_biome &lt;- cases_environment_data %&gt;% group_by(date, year, biome) %&gt;% summarise(incidence = sum(casos, na.rm = TRUE) / sum(population, na.rm = TRUE) * 10000, .groups = &#39;drop&#39;) # Calculate incidence rates by Köppen climate zone incidence_rates_by_koppen &lt;- cases_environment_data %&gt;% group_by(date, koppen) %&gt;% summarise(incidence = sum(casos, na.rm = TRUE) / sum(population, na.rm = TRUE) * 10000, .groups = &#39;drop&#39;) # Plot incidence trends by biome p_biome &lt;- ggplot(incidence_rates_by_biome) + geom_line(aes(x = date, y = incidence, group = biome), color = &#39;black&#39;, linewidth = 1.5) + geom_line(aes(x = date, y = incidence, group = biome, color = biome), linewidth = 1) + scale_color_brewer(palette = &#39;Set3&#39;) + facet_wrap(~biome, nrow = 3) + theme_bw() + labs(title = &quot;Dengue Incidence by Biome&quot;, x = &quot;Date&quot;, y = &quot;Incidence per 10,000&quot;) + theme(legend.position = &quot;none&quot;) # Plot incidence trends by Köppen climate zone p_koppen &lt;- ggplot(incidence_rates_by_koppen) + geom_line(aes(x = date, y = incidence, group = koppen), color = &#39;black&#39;, linewidth = 1.5) + geom_line(aes(x = date, y = incidence, group = koppen, color = koppen), linewidth = 1) + scale_color_brewer(palette = &#39;Paired&#39;) + facet_wrap(~koppen, nrow = 3) + theme_bw() + labs(title = &quot;Dengue Incidence by Köppen Climate Zone&quot;, x = &quot;Date&quot;, y = &quot;Incidence per 10,000&quot;) + theme(legend.position = &quot;none&quot;) p_biome p_koppen 9.9 Seasonal Analysis: Weekly Patterns by Biome Dengue transmission is highly seasonal, influenced by rainfall, temperature, and mosquito breeding cycles. Let’s examine the seasonal patterns across different biomes: # Prepare seasonal analysis (excluding incomplete 2025 data) seasonal_data &lt;- incidence_rates_by_biome %&gt;% filter(year != 2025) %&gt;% mutate(week = week(date)) # Calculate average weekly incidence by biome seasonal_incidence_by_biome &lt;- seasonal_data %&gt;% group_by(biome, week) %&gt;% summarise(mean_incidence = mean(incidence, na.rm = TRUE), .groups = &#39;drop&#39;) # Plot seasonal patterns ggplot(seasonal_incidence_by_biome, aes(x = week, y = mean_incidence, color = biome)) + geom_line(size = 1.2) + labs(title = &quot;Seasonal Dengue Patterns by Biome&quot;, subtitle = &quot;Average weekly incidence across all years (2010-2024)&quot;, x = &quot;Week of Year&quot;, y = &quot;Mean Incidence per 10,000 Population&quot;, color = &quot;Biome&quot;) + scale_color_brewer(palette = &quot;Dark2&quot;) + theme_minimal() + scale_x_continuous(limits = c(1, 53), breaks = seq(1, 53, by = 4), labels = seq(1, 53, by = 4)) + theme(legend.position = &quot;bottom&quot;) ## Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0. ## ℹ Please use `linewidth` instead. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was ## generated. Now this is an interesting one. Here you see the total number of incidence rates per calendar week between 2010 and 2024 per biome. The absolute number is not really important, which I assume heavily depends on how populated these areas are. What is interesting though, is that in the final few weeks of the year, cases rise in the cerrado, pantanal and mata atlantica, while cases in caatinga and pampa rise much later (week 5 and 9 respectively). We can probably have a look whether there are any climate variables that rise and fall around the same periods in the same areas, and whether this pattern is observed over multiple years or is just an artefact of the aggregation of all years combined. 9.10 Discussion and Key Findings This comprehensive analysis of dengue patterns in Brazil reveals several important epidemiological insights: Temporal Patterns: - Clear epidemic cycles with major outbreaks occurring every few years - Strong seasonal patterns with peak transmission during warmer, wetter months - Variation in seasonal timing across different biomes and climate zones Spatial Patterns: - Heterogeneous distribution across Brazil’s diverse geography - Higher incidence rates in tropical and subtropical regions - Urban and peri-urban areas showing different transmission dynamics Environmental Associations: - Different biomes exhibit distinct seasonal patterns and incidence levels - Climate zones show varying susceptibility to dengue transmission - Large-scale climate oscillations may influence epidemic timing and intensity This analysis demonstrates the complexity of dengue epidemiology and the importance of considering environmental, temporal, and spatial factors in understanding disease transmission patterns. Such insights are crucial for developing targeted prevention strategies and early warning systems for dengue control in Brazil. 9.11 Further Analysis Opportunities This dataset provides numerous opportunities for additional epidemiological investigations: - Statistical modeling of climate-disease associations - Time series analysis and forecasting - Spatial clustering analysis - Risk factor identification and quantification - Evaluation of control interventions "],["modelling.html", "Chapter 10 Modelling 10.1 Compartmental models 10.2 Time series - Forecasting 10.3 Using and applying models", " Chapter 10 Modelling Models in epidemiology are simplified representations of reality that help us understand, explain, and predict how diseases spread. They take key aspects of complex systems—like populations, contacts, or time—and turn them into frameworks we can analyze. By doing so, models provide a bridge between data and decision-making in public health. 10.1 Compartmental models 10.1.1 SIR model in theory Compartmental models divide a population into distinct groups, or compartments, each governed by specific rules that describe how individuals move between compartments over time. In epidemiology, one of the most common models is the SIR model, which divides the population into three compartments: Susceptible (S): individuals who can contract the disease Infectious (I): individuals who are currently infected and can transmit the disease Recovered (R): individuals who have recovered and are no longer infectious Individuals move from S to I (when infected), and from I to R (when they recover). This progression can be described using either discrete-time or continuous-time equations. 10.1.1.1 Discrete-Time Formulation The discrete-time version of the SIR model is expressed as: \\[ \\begin{aligned} S_{t+1} &amp;= S_t - \\beta S_t I_t \\\\ I_{t+1} &amp;= I_t + \\beta S_t I_t - \\gamma I_t \\\\ R_{t+1} &amp;= R_t + \\gamma I_t \\end{aligned} \\] Here, \\(S_{t+1}\\) is the number of susceptible individuals at the next time step, determined by subtracting those who become infected. \\(I_{t+1}\\) increases with new infections and decreases as people recover. \\(R_{t+1}\\) increases with recoveries. 10.1.1.2 Continuous-Time Formulation (ODEs) For more refined, continuous-time modeling, we use ordinary differential equations: \\[ \\begin{aligned} \\frac{dS}{dt} &amp;= -\\beta S I \\\\ \\frac{dI}{dt} &amp;= \\beta S I - \\gamma I \\\\ \\frac{dR}{dt} &amp;= \\gamma I \\end{aligned} \\] These represent the rate of change of each compartment over time. Where: - \\(\\beta\\) is the transmission rate - \\(\\gamma\\) is the recovery rate - \\(N = S + I + R\\) is the total population 10.1.2 SIR model in R Despite its simplicity, the SIR model can be surprisingly effective. During the COVID-19 pandemic, many government advisors based their projections on variations of this basic framework. As an example, consider an influenza outbreak that occurred in 1978 at a boarding school in England. The on-site physician documented how many students were sick (in bed) and how many were recovering (convalescent) each day. This dataset is included in the outbreaks R package, allowing us to study it directly. library(outbreaks) data(&quot;influenza_england_1978_school&quot;) outbreak_data &lt;- influenza_england_1978_school head(outbreak_data) ## date in_bed convalescent ## 1 1978-01-22 3 0 ## 2 1978-01-23 8 0 ## 3 1978-01-24 26 0 ## 4 1978-01-25 76 0 ## 5 1978-01-26 225 9 ## 6 1978-01-27 298 17 outbreak_data_long &lt;- pivot_longer(outbreak_data, cols = c(in_bed, convalescent), names_to = &quot;group&quot;, values_to = &quot;count&quot;) ggplot(outbreak_data_long, aes(x = date, y = count, color = group)) + geom_line(linewidth = 1.2) + geom_point(color = &quot;black&quot;, size = 2) + scale_x_date(breaks = outbreak_data_long$date, date_labels = &quot;%b %d&quot;) + labs(title = &quot;Influenza Outbreak (1978 Boarding School)&quot;, x = &quot;&quot;, y = &quot;Number of Students&quot;, color = &quot;Status&quot;) + theme_minimal() + theme(axis.text.x = element_text(angle = 45, hjust = 1)) Now let’s implement the SIR model in R: sir_model &lt;- function(number_of_days, BETA, GAMMA, dt = 1) { N &lt;- 763 I0 &lt;- 1 S0 &lt;- N - I0 R0 &lt;- 0 times &lt;- seq(0, number_of_days, by = dt) n_steps &lt;- length(times) sirdata &lt;- data.frame( t = times, S = numeric(n_steps), I = numeric(n_steps), R = numeric(n_steps) ) sirdata[1, ] &lt;- c(0, S0, I0, R0) for (tt in 2:n_steps) { S_prev &lt;- sirdata$S[tt - 1] I_prev &lt;- sirdata$I[tt - 1] R_prev &lt;- sirdata$R[tt - 1] new_infections &lt;- BETA * S_prev * I_prev * dt new_recoveries &lt;- GAMMA * I_prev * dt new_infections &lt;- min(new_infections, S_prev) new_recoveries &lt;- min(new_recoveries, I_prev) S_new &lt;- S_prev - new_infections I_new &lt;- I_prev + new_infections - new_recoveries R_new &lt;- R_prev + new_recoveries sirdata[tt, ] &lt;- c(times[tt], S_new, I_new, R_new) } return(sirdata) } output = sir_model(14, 0.0026, 0.565, 0.1) sirdata_long = pivot_longer(output, cols = c(&quot;S&quot;, &quot;I&quot;, &quot;R&quot;), names_to = &quot;compartment&quot;, values_to = &quot;count&quot;) ggplot(sirdata_long, aes(x = t, y = count, color = compartment)) + geom_line(linewidth = 1.2) + labs(title = &quot;Influenza Outbreak (1978 Boarding School)&quot;, x = &quot;&quot;, y = &quot;Number of Students&quot;, color = &quot;Status&quot;) + theme_minimal() Let’s now compare the observed in_bed data with the simulated number of infectious individuals: sir_output &lt;- sir_model(14, 0.0026, 0.565, dt = 0.1) sir_daily &lt;- sir_output %&gt;% group_by(day = floor(t)) %&gt;% summarize(I_simulated = mean(I)) observed_data &lt;- outbreak_data %&gt;% mutate(day = as.numeric(date - min(date))) %&gt;% select(day, in_bed) plot_data &lt;- left_join(observed_data, sir_daily, by = &quot;day&quot;) %&gt;% pivot_longer(cols = c(in_bed, I_simulated), names_to = &quot;type&quot;, values_to = &quot;count&quot;) ggplot(plot_data, aes(x = day, y = count, color = type)) + geom_line(linewidth = 1.2) + geom_point(size = 2, alpha = 0.8) + scale_x_continuous(breaks = 0:max(plot_data$day)) + labs(title = &quot;Observed vs Simulated Infections&quot;, x = &quot;Days Since Outbreak Start&quot;, y = &quot;Number of Students&quot;, color = &quot;Data Type&quot;) + theme_minimal() + theme(axis.text.x = element_text(angle = 0)) These compartmental models can be done using population-based equations, like here. One funadmental issue with those though, is that it is very hard to model heterogeneity in the population. In reality, we have clusters of people that are not vaccinated for example. Using the equations above, and adding a V compartment, the model would make the assumption that the vaccinated and unvaccinated are spread evently. That’s why sometimes these models are implemented using agent based models, where individuals are modelled. The mathematics becomes a bit more complicated, but these models allow us to do much more. Below you will find two examples of such models. For the following one, I modelled people moving around two dedicated centers, two cities if you will. There’s very little, but some, movement between the cities. The infection start in city 1 and later also spreads to city 2. 10.2 Time series - Forecasting Forecasting plays a crucial role in infectious disease epidemiology by enabling short-term projections of case counts. These projections help guide timely public health decisions, especially when diseases exhibit seasonal or trend-driven behavior. In this section, we compare two popular time series approaches: Holt-Winters exponential smoothing, which models level, trend, and seasonality using exponentially weighted averages. ARIMA (AutoRegressive Integrated Moving Average), a flexible class of models that can account for autoregressive lags, differencing (trends), and moving average components, including seasonal extensions (SARIMA). Unlike mechanistic models (e.g., SIR), these statistical methods do not rely on assumptions about transmission mechanisms. Instead, they learn from the patterns and structure of historical data to forecast future values. We apply both models to weekly national tick-borne encephalitis (TBE) cases and generate forecasts for the year following a specified historical cutoff. library(dplyr) library(ggplot2) library(forecast) ## Registered S3 method overwritten by &#39;quantmod&#39;: ## method from ## as.zoo.data.frame zoo library(lubridate) library(patchwork) # --- Load and preprocess data --- tbe_local &lt;- read.csv(&quot;data/tick_borne_encephalitis.csv&quot;) tbe_national &lt;- tbe_local %&gt;% mutate(timestamp = as.Date(timestamp)) %&gt;% group_by(timestamp) %&gt;% summarise(cases = sum(cases), .groups = &quot;drop&quot;) # --- Forecasting setup --- year &lt;- 2018 # Change to define cutoff year tbe_national_context &lt;- tbe_national %&gt;% filter(year(timestamp) &lt;= year) tbe_national_future &lt;- tbe_national %&gt;% filter(year(timestamp) == year + 1) %&gt;% arrange(timestamp) # ---- Create time series object ---- context_ts &lt;- ts(tbe_national_context$cases, frequency = 52) # ---- Holt-Winters model ---- hw_model &lt;- HoltWinters(context_ts) forecast_hw &lt;- forecast(hw_model, h = 52) predicted_df_hw &lt;- data.frame( timestamp = seq.Date(from = as.Date(paste0(year + 1, &quot;-01-01&quot;)), by = &quot;week&quot;, length.out = 52), cases = as.numeric(forecast_hw$mean), type = &quot;predicted_hw&quot; ) # ---- ARIMA model ---- arima_model &lt;- auto.arima(context_ts, seasonal = TRUE) forecast_arima &lt;- forecast(arima_model, h = 52) predicted_df_arima &lt;- data.frame( timestamp = seq.Date(from = as.Date(paste0(year + 1, &quot;-01-01&quot;)), by = &quot;week&quot;, length.out = 52), cases = as.numeric(forecast_arima$mean), type = &quot;predicted_arima&quot; ) # ---- Combine all data for plotting ---- context_labeled &lt;- tbe_national_context %&gt;% mutate(type = &quot;historical&quot;) future_labeled &lt;- tbe_national_future %&gt;% mutate(type = &quot;observed&quot;) combined_data &lt;- bind_rows(context_labeled, future_labeled, predicted_df_hw, predicted_df_arima) # ---- Plot full timeline ---- main_plot &lt;- ggplot(combined_data, aes(x = timestamp, y = cases, color = type)) + geom_line(linewidth = 1.2) + scale_color_manual(values = c( &quot;historical&quot; = &quot;#1f77b4&quot;, &quot;observed&quot; = &quot;#2ca02c&quot;, &quot;predicted_hw&quot; = &quot;#d62728&quot;, &quot;predicted_arima&quot; = &quot;#ff7f0e&quot; )) + theme_classic() + labs( title = paste(&quot;TBE Weekly Cases: Forecasting&quot;, year + 1), x = &quot;Time&quot;, y = &quot;Cases&quot;, color = &quot;Data Type&quot; ) + theme( legend.position = &quot;top&quot;, plot.title = element_text(face = &quot;bold&quot;, size = 16) ) # ---- Plot zoomed forecast year ---- zoom_plot &lt;- combined_data %&gt;% filter(year(timestamp) == year + 1) %&gt;% ggplot(aes(x = timestamp, y = cases, color = type)) + geom_line(linewidth = 1.2) + scale_color_manual(values = c( &quot;historical&quot; = &quot;#1f77b4&quot;, &quot;observed&quot; = &quot;#2ca02c&quot;, &quot;predicted_hw&quot; = &quot;#d62728&quot;, &quot;predicted_arima&quot; = &quot;#ff7f0e&quot; )) + theme_classic() + labs( title = paste(&quot;Zoom: Forecasts vs Observed in&quot;, year + 1), x = &quot;Week&quot;, y = &quot;Cases&quot; ) + theme( legend.position = &quot;none&quot;, plot.title = element_text(face = &quot;bold&quot;) ) # ---- Combine and show ---- main_plot / zoom_plot + plot_layout(heights = c(2, 1)) This analysis demonstrates how different statistical forecasting methods—Holt-Winters vs ARIMA—perform on the same epidemiological time series. While Holt-Winters is intuitive and captures seasonality well, ARIMA can offer greater flexibility in modeling autocorrelated structures. Comparing their predictions side-by-side allows us to evaluate model fit and choose an approach best suited for early warning or planning systems. 10.3 Using and applying models "],["communication-and-sharing-of-code-and-data.html", "Chapter 11 Communication and sharing of code and data 11.1 Writing code: best practices 11.2 Github 11.3 Reports using Markdown 11.4 Open science 11.5 Communities and forums 11.6 How to use Chat-GPT", " Chapter 11 Communication and sharing of code and data 11.1 Writing code: best practices 11.2 Github 11.3 Reports using Markdown 11.4 Open science 11.5 Communities and forums 11.6 How to use Chat-GPT "],["recommended-reading.html", "Chapter 12 Recommended Reading 12.1 Recommended textbooks and courses 12.2 Datasources 12.3 Useful packages", " Chapter 12 Recommended Reading 12.1 Recommended textbooks and courses https://epirhandbook.com/en/new_pages/ggplot_basics.html: great pieces on ggplot (chapter 30) 12.2 Datasources 12.3 Useful packages "],["references.html", "References", " References Chippaux, J. P., &amp; Chippaux, A. (2018). Yellow fever in africa and the americas: A historical and epidemiological perspective. In Journal of Venomous Animals and Toxins Including Tropical Diseases (Vol. 24). BioMed Central Ltd. https://doi.org/10.1186/s40409-018-0162-y Schrijver, S. D., Vanhulle, E., Ingenbleek, A., Alexakis, L., Johannesen, C. K., Broberg, E. K., Harvala, H., Fischer, T. K., Benschop, K. S. M., Albert, J., Allen, N., Andreoletti, L., Atabani, S., Auvray, C., Bart, A., Berengua, C., Berginc, N., Bisseux, M., Boon, H., … Zanetti, L. (2025). Epidemiological and clinical insights into enterovirus circulation in europe, 2018-2023: A multicenter retrospective surveillance study. Journal of Infectious Diseases, 232, e104–e115. https://doi.org/10.1093/infdis/jiaf179 "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
